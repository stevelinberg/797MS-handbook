% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{book}
\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={797ML Handbook},
  pdfauthor={Steve Linberg},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\usepackage{booktabs}
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage[]{natbib}
\bibliographystyle{plainnat}

\title{797ML Handbook}
\author{Steve Linberg}
\date{2022-04-17}

\begin{document}
\maketitle

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\hypertarget{about}{%
\chapter{About}\label{about}}

This book is being written as part of a final project for 797ML at UMass
Amherst, spring 2022. It contains a simple reference and breakdown for a
couple of dozen core methods used in machine learning.

The intent is twofold:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Serve as a reference for the basics of the material covered in the class, using language and examples that are as simple as possible to explain the core concepts and how to do them;
\item
  Force myself to learn these techniques better by carrying out the above.
\end{enumerate}

The main purpose of this work is to be \emph{simple}, not to be \emph{comprehensive}. We won't cover every facet of every technique, or every possible permutations of outcomes. The goal is to simply express the broad strokes and core concepts in a way that can be easily remembered, and to serve as a jumping-off point when more information is needed.

\hypertarget{authoring-guidelines}{%
\section{Authoring guidelines}\label{authoring-guidelines}}

The goal is to have no more than a few short paragraphs for each section, and to keep each explanation of the meanings of outcome variables to one sentence each.

Each chapter will have a \href{https://en.wikipedia.org/wiki/Wikipedia:Too_long;_didn\%27t_read}{TL;DR} section at the top with \emph{one-sentence answers} to following questions:

\begin{itemize}
\tightlist
\item
  What it does (answer the question starting with ``It\ldots{}'')
\item
  When to do it
\item
  How to do it
\item
  How to assess it
\end{itemize}

and it will be possible to skim through the book, just surfing the TL;DR at the top of each chapter, and get a reasonable overview without reading the rest. The rest of each chapter will deliver more fleshed-out, but \emph{short}, answers to the same questions, covering the basics and the most general concepts.

\hypertarget{contact}{%
\section{Contact}\label{contact}}

Steve Linberg: \href{mailto:steve@slinberg.net}{\nolinkurl{steve@slinberg.net}} \textbar\textbar{} \url{https://slinberg.net}

Project home: \url{https://stevelinberg.github.io/797MS-handbook/}

Github repo: \url{https://github.com/stevelinberg/797MS-handbook}

\hypertarget{resources}{%
\section{Resources}\label{resources}}

A lot of the material from this work is from the class textbook, \citet{ISLR}. I also find UNC geneticist Josh Starmer's \href{https://www.youtube.com/channel/UCtYLUTtgS3k1Fg4y5tAhLbw}{StatQuest} video series on YouTube to be immensely helpful for simple explanations of statistics and machine learning concepts. His website \href{https://statquest.org}{statquest.org} has many additional useful resources.

\hypertarget{simple-linear-regression}{%
\chapter{Simple Linear Regression}\label{simple-linear-regression}}

\hypertarget{tldr}{%
\section{TL;DR}\label{tldr}}

\begin{description}
\tightlist
\item[What it does]
Looks to see how well a single predictor variable predicts an outcome, like \emph{how well do years of education predict salary?}
\item[When to do it]
When you want to see if pretty much the simplest possible model provides enough of an explanation of variance for your purposes
\item[How to do it]
With the \texttt{lm()} function, among other ways
\item[How to assess it]
Look for a significant \(p\)-value for the predictor, and a reasonable \(R^2\)
\item[Note]
``Linear'' does not necessarily mean ``straight'' in this context; a linear regression line can curve.
\end{description}

\hypertarget{what-it-does}{%
\section{What it does}\label{what-it-does}}

Simple linear regression is where it all begins; among the simplest of all of the regression techniques in analysis, which attempts to estimate a slope and an intercept line for a set of observations using a single predictor variable \(X\) and an output variable \(Y\). It uses ordinary least squares (OLS) to build its model, looking for the line through the mean of \(X\) and \(Y\) that has the smallest sum of squares between the predicted and observed values.



\begin{figure}
\centering
\includegraphics{images/3_1.png}
\caption{\label{fig:img-slr1}Simple linear regression plot (source: \citet{ISLR}, p.~62)}
\end{figure}

The figure above shows a plot of a simple linear regression, attempting to use the variable \texttt{TV} to predict \texttt{Sales.} The blue line is the line defined by the regression with its \(Y\) intercept and slope; the red dots are the actual observations of \texttt{Sales} for each measure of the predictor \texttt{TV} on the \(X\) axis. The thin blue lines are the error in the prediction.

\hypertarget{when-to-do-it}{%
\section{When to do it}\label{when-to-do-it}}

It is a simple first step for looking at data to see if there is an easy single-variable model that does a reasonable job predicting outcomes using one predictor variable. Sometimes, it can be good enough! It has the advantage of being easy to execute, to understand and to communicate, and the value of these factors should not be underestimated. Communicating with non-specialists is an important aspect of a data scientist's job.

Linear regression requires a dataset with a continuous outcome variable; it is easiest and most effective if the predictor variable is also numeric, whether continuous or discrete. It is possible to do linear regression with non-numeric predictors, such as true/false or ordered responses, by converting the predictors to a numeric scale.

\hypertarget{how-to-do-it}{%
\section{How to do it}\label{how-to-do-it}}

This example from \citet{ISLR} shows a simple linear regression of \textbf{medv} onto \textbf{lstat}, attempting to predict median housing prices from percentage of ``lower-status'' population in the Boston data set from the late 1970s.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lm.fit }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(medv }\SpecialCharTok{\textasciitilde{}}\NormalTok{ lstat, }\AttributeTok{data =}\NormalTok{ Boston)}
\FunctionTok{plot}\NormalTok{(Boston}\SpecialCharTok{$}\NormalTok{lstat, Boston}\SpecialCharTok{$}\NormalTok{medv)}
\FunctionTok{abline}\NormalTok{(lm.fit, }\AttributeTok{lwd =} \DecValTok{3}\NormalTok{, }\AttributeTok{col =} \StringTok{"red"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{_main_files/figure-latex/unnamed-chunk-3-1.pdf}

The results of the regression are stored in the output of \texttt{lm()}, and may be viewed with \texttt{summary()}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(lm.fit)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = medv ~ lstat, data = Boston)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -15.168  -3.990  -1.318   2.034  24.500 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(>|t|)    
## (Intercept) 34.55384    0.56263   61.41   <2e-16 ***
## lstat       -0.95005    0.03873  -24.53   <2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 6.216 on 504 degrees of freedom
## Multiple R-squared:  0.5441, Adjusted R-squared:  0.5432 
## F-statistic: 601.6 on 1 and 504 DF,  p-value: < 2.2e-16
\end{verbatim}

\hypertarget{how-to-assess-it}{%
\section{How to assess it}\label{how-to-assess-it}}

The output contains a lot of information, but the key points are:

\begin{itemize}
\tightlist
\item
  \texttt{Adjusted\ R-squared}: 0.5432 is the percentage of the variation in the model that is explained by the fit's prediction, compared to just looking at how much the observations vary from their own average with no predictor variable.
\item
  \texttt{Residual\ standard\ error}: 6.216 is ``the average amount that the response will deviate from the true regression line'', or the standard deviation of the error. In this case, since the unit of \texttt{medv} is in thousands of dollars, it means the average prediction will be off by \$6,216.
\item
  \texttt{p-value}: \textless{} 2.2e-16 (basically zero) means that the model is (very) statistically significant, with a near-zero percent chance that the data in this set could have resulted from a random draw from the (unknown) population if there were no relationship between \texttt{medv} and \texttt{lstat}.
\item
  \texttt{Estimated\ intercept} of 34.55 is the \(Y\) intercept on the graph, or the estimated value of \texttt{medv} when \texttt{lstat} is zero.
\item
  \texttt{Estimated\ (lstat)}: -0.95 is the estimated coefficient of \texttt{lstat}, or the amount that \texttt{medv} changes by for each unit change in \texttt{lstat}.
\end{itemize}

The intercept and coefficient are \(\beta_0\) and \(\beta_1\), respectively, of the linear regression formula

\[\hat{y} = \hat{\beta_0} + \hat{\beta_1}x\]
Substituting the coefficients and variables, we transform this to:

\[\widehat{\text{medv}} = 34.55 - 0.95 \times \text{lstat}\]

The low p-value shows that there is (almost) definitely a relationship between \texttt{medv} and \texttt{lstat}, but the \(R^2\) of 0.54 shows that the model only explains slightly more than half of the variation in the data. It's not a bad start, but we would probably want to find a model that explains more of it.

\hypertarget{where-to-learn-more}{%
\section{Where to learn more}\label{where-to-learn-more}}

\begin{itemize}
\tightlist
\item
  Chapter 3 - 3.1 in \citet{ISLR}
\item
  \href{https://www.youtube.com/watch?v=nk2CQITm_eo}{StatQuest: Linear Regression} - this goes into good depth on the meaning of the \(F\) statistic as well, and how it used to calculate the \(p\) value.
\end{itemize}

\hypertarget{notes}{%
\section{Notes}\label{notes}}

You can also do a scatterplot in ggplot2 and add a regression line with \texttt{geom\_smooth()}; it doesn't show the coefficients or other output information, but it gives a quick visual that's a little prettier than the base R plot:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ Boston, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ lstat, }\AttributeTok{y =}\NormalTok{ medv)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{alpha =} \FloatTok{0.5}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{method =} \StringTok{"lm"}\NormalTok{, }\AttributeTok{formula =}\NormalTok{ y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x) }\SpecialCharTok{+}
  \FunctionTok{geom\_hline}\NormalTok{(}\AttributeTok{yintercept =} \FunctionTok{mean}\NormalTok{(Boston}\SpecialCharTok{$}\NormalTok{medv), }\AttributeTok{linetype =} \StringTok{"dashed"}\NormalTok{, }\AttributeTok{color =} \StringTok{"red"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_vline}\NormalTok{(}\AttributeTok{xintercept =} \FunctionTok{mean}\NormalTok{(Boston}\SpecialCharTok{$}\NormalTok{lstat), }\AttributeTok{linetype =} \StringTok{"dashed"}\NormalTok{, }\AttributeTok{color =} \StringTok{"darkgreen"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{_main_files/figure-latex/unnamed-chunk-5-1.pdf}

The means of \texttt{lstat} and \texttt{medv} are shown with dashed red and green lines, showing that the regression line goes through the center of the data.

\hypertarget{multiple-linear-regression}{%
\chapter{Multiple Linear Regression}\label{multiple-linear-regression}}

\hypertarget{tldr-1}{%
\section{TL;DR}\label{tldr-1}}

\begin{description}
\tightlist
\item[What it does]
Looks to see how well multiple predictor variables predict an outcome, like \emph{how well do years of education and age predict salary?}
\item[When to do it]
When a \protect\hyperlink{simple-linear-regression}{simple linear regression} doesn't provide a good enough explanation of variance, and you want to see if adding additional variables provides a better one
\item[How to do it]
With the \texttt{lm()} function, utilizing more than one predictor
\item[How to assess it]
Look for significant \(p\)-values for the predictors, and a reasonable adjusted-\(R^2\)
\end{description}

\hypertarget{what-it-does-1}{%
\section{What it does}\label{what-it-does-1}}

Multiple linear regression is the first natural extension of simple linear regression. It allows for more than one predictor variable to be specified. It is also possible to combine predictors in interactions, to find out if combinations of predictors have different effects than simply adding them to the model. XXX explain/demo

\hypertarget{when-to-do-it-1}{%
\section{When to do it}\label{when-to-do-it-1}}

Use multiple linear regression when a simple linear regression doesn't provide a good enough explanation of the variance you're observing, and you want to see if adding more predictors provides a better fit. Typically, this would be in response to either a low \(R^2\) that leaves a lot of unexplained variance, or even just a visual conclusion drawn from seeing a plot of a linear model with an unsatisfactory regression line.

\hypertarget{how-to-do-it-1}{%
\section{How to do it}\label{how-to-do-it-1}}

Use the same \texttt{lm()} function, with more than one predictor in the formula.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lm.fit }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(medv }\SpecialCharTok{\textasciitilde{}}\NormalTok{ lstat }\SpecialCharTok{+}\NormalTok{ age, }\AttributeTok{data =}\NormalTok{ Boston)}
\FunctionTok{plot}\NormalTok{(Boston}\SpecialCharTok{$}\NormalTok{lstat, Boston}\SpecialCharTok{$}\NormalTok{medv)}
\FunctionTok{abline}\NormalTok{(lm.fit, }\AttributeTok{lwd =} \DecValTok{3}\NormalTok{, }\AttributeTok{col =} \StringTok{"red"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{_main_files/figure-latex/unnamed-chunk-7-1.pdf}

As with simplr linear regression, the results of the regression are stored in the output of \texttt{lm()}, and may be viewed with \texttt{summary()}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(lm.fit)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = medv ~ lstat + age, data = Boston)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -15.981  -3.978  -1.283   1.968  23.158 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(>|t|)    
## (Intercept) 33.22276    0.73085  45.458  < 2e-16 ***
## lstat       -1.03207    0.04819 -21.416  < 2e-16 ***
## age          0.03454    0.01223   2.826  0.00491 ** 
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 6.173 on 503 degrees of freedom
## Multiple R-squared:  0.5513, Adjusted R-squared:  0.5495 
## F-statistic:   309 on 2 and 503 DF,  p-value: < 2.2e-16
\end{verbatim}

All of the same parameters are there, with the addition of one coefficient line for each parameter specified in the model. Each coefficient has its own estimate, standard error, \(t\) value and \(p\) value, and should be evaluated independently.

The main task is to get the ideal subset of parameters that provide the best fit, which can involve a lot of trial and error. \emph{Forward selection} involves starting with a null model (no parameters) and trying all available parameters to see which has the lowest p-value, and adding that and continuing until the parameters are no longer significant or don't reduce the adjusted \(R^2\); \emph{Backward selection} starts with all parameters and removes the least significant one at a time until all remaining parameters are significant; \emph{Mixed selection} starts with forward selection but also removes parameters that lose their significance along the way.

Other tidbits:

\begin{itemize}
\tightlist
\item
  \emph{interaction terms} can be specified with syntax like \texttt{lstat\ *\ age}, which adds \texttt{lstat}, \texttt{age} and \texttt{lstat\ *\ age} as predictors; if an interaction is significant but an individual predictor from the interaction isn't, it should still be left in the model
\item
  \emph{nonlinear transformations} like \(\text{age}^2\) can be added by escaping them inside the \texttt{I()} function, like \texttt{I(age\^{}2)}, to escape the \texttt{\^{}} character inside the formula
\end{itemize}

\hypertarget{how-to-assess-it-1}{%
\section{How to assess it}\label{how-to-assess-it-1}}

With the exception of the additional information for each parameter, the assessment is the same as for \protect\hyperlink{simple-linear-regression}{simple linear regression}.

In the example above, we see that the addition of \texttt{age} only added a tiny bit of improvement to the model from the simple linear regression on just \texttt{lstat}. Other variables, or combinations of variables, might do a better job.

\hypertarget{where-to-learn-more-1}{%
\section{Where to learn more}\label{where-to-learn-more-1}}

\begin{itemize}
\tightlist
\item
  Chapter 3.2 in \citet{ISLR}
\end{itemize}

\hypertarget{logistic-regression}{%
\chapter{Logistic Regression}\label{logistic-regression}}

\hypertarget{tldr-2}{%
\section{TL;DR}\label{tldr-2}}

\begin{description}
\tightlist
\item[What it does]
Models the \emph{probability} that an observation falls into one of two categories
\item[When to do it]
When you are trying to predict a categorical variable with two possible outcomes, like true/false, instead of something continuous like in linear regression
\item[How to do it]
With the \texttt{glm()} function, specifying \texttt{family\ =\ "binomial"}
\item[How to assess it]
Look for a significant \(p\)-value for the predictor, and assess its accuracy against training data
\end{description}

\hypertarget{what-it-does-2}{%
\section{What it does}\label{what-it-does-2}}

Logistic regression is used to make a prediction about whether an observation falls into one of two categories. This is an alternative to linear regression, which predicts a continuous outcome. A logistic regression results in a model that gives the probability of Y given X.



\begin{figure}
\includegraphics[width=0.5\linewidth]{images/4_1} \caption{Linear regression plot (source: \href{https://youtu.be/yIYKR4sgzI8?t=222}{StatQuest})}\label{fig:img-lr1}
\end{figure}

\hypertarget{when-to-do-it-2}{%
\section{When to do it}\label{when-to-do-it-2}}

As the TL;DR says: When you are trying to predict a categorical variable with two possible outcomes, like true/false, instead of something continuous.

\hypertarget{how-to-do-it-2}{%
\section{How to do it}\label{how-to-do-it-2}}

First, it's important to note that logistic regression will give better results when the model is fit using \emph{held out} or \emph{training} data, and then tested against other data, to help avoid overfitting. It is not required, but it's recommended where possible.

First, before that, fit a model using \texttt{glm()}, making sure in this case that we have a categorical value to test for:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data}\NormalTok{(Boston)}
\NormalTok{boston }\OtherTok{\textless{}{-}}\NormalTok{ Boston }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}
    \CommentTok{\# Create the categorical crim\_above\_med response variable}
    \AttributeTok{crim\_above\_med =} \FunctionTok{as.factor}\NormalTok{(}\FunctionTok{ifelse}\NormalTok{(crim }\SpecialCharTok{\textgreater{}} \FunctionTok{median}\NormalTok{(crim), }\StringTok{"Yes"}\NormalTok{, }\StringTok{"No"}\NormalTok{)),}
    \CommentTok{\# Also make a numeric version of crim\_above\_med for the plot}
    \AttributeTok{crim\_above\_med\_num =} \FunctionTok{ifelse}\NormalTok{(crim }\SpecialCharTok{\textgreater{}} \FunctionTok{median}\NormalTok{(crim), }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{)}
\NormalTok{  )}
\NormalTok{boston.fits }\OtherTok{\textless{}{-}} \FunctionTok{glm}\NormalTok{(crim\_above\_med }\SpecialCharTok{\textasciitilde{}}\NormalTok{ nox, }\AttributeTok{data =}\NormalTok{ boston, }\AttributeTok{family =}\NormalTok{ binomial)}
\FunctionTok{summary}\NormalTok{(boston.fits)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## glm(formula = crim_above_med ~ nox, family = binomial, data = boston)
## 
## Deviance Residuals: 
##      Min        1Q    Median        3Q       Max  
## -2.27324  -0.37245  -0.06847   0.39620   2.53124  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(>|z|)    
## (Intercept)  -15.818      1.386  -11.41   <2e-16 ***
## nox           29.365      2.599   11.30   <2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 701.46  on 505  degrees of freedom
## Residual deviance: 320.39  on 504  degrees of freedom
## AIC: 324.39
## 
## Number of Fisher Scoring iterations: 6
\end{verbatim}

We can see the that \(p\) value for the \texttt{nox} variable is nearly 0, suggesting a strong association between \texttt{nox} and \texttt{crim\_above\_med}.

Now split the data into training and test sets, using, say, 80\% of the data for training:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1235}\NormalTok{)}
\NormalTok{boston.training }\OtherTok{\textless{}{-}} \FunctionTok{rep}\NormalTok{(}\ConstantTok{FALSE}\NormalTok{, }\FunctionTok{nrow}\NormalTok{(boston))}
\NormalTok{boston.training[}\FunctionTok{sample}\NormalTok{(}\FunctionTok{nrow}\NormalTok{(boston), }\FunctionTok{nrow}\NormalTok{(boston) }\SpecialCharTok{*} \FloatTok{0.8}\NormalTok{)] }\OtherTok{\textless{}{-}} \ConstantTok{TRUE}
\NormalTok{boston.test }\OtherTok{\textless{}{-}} \SpecialCharTok{!}\NormalTok{boston.training}
\end{Highlighting}
\end{Shaded}

Fit the above model to the training data:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{boston.fits }\OtherTok{\textless{}{-}}
  \FunctionTok{glm}\NormalTok{(}
\NormalTok{    crim\_above\_med }\SpecialCharTok{\textasciitilde{}}\NormalTok{ nox,}
    \AttributeTok{data =}\NormalTok{ boston,}
    \AttributeTok{subset =}\NormalTok{ boston.training,}
    \AttributeTok{family =}\NormalTok{ binomial}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

And use the fit to predict the test data with \texttt{predict()}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{boston.probs }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(boston.fits, boston[boston.test,], }\AttributeTok{type =} \StringTok{"response"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{how-to-assess-it-2}{%
\section{How to assess it}\label{how-to-assess-it-2}}

Assess the accuracy of the predictions by building a confusion matrix, using the same values (``No'' and ``Yes'' in this case) as contained in the outcome variable we want to predict:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{boston.pred }\OtherTok{\textless{}{-}} \FunctionTok{rep}\NormalTok{(}\StringTok{"No"}\NormalTok{, }\FunctionTok{sum}\NormalTok{(boston.test))}
\NormalTok{boston.pred[boston.probs }\SpecialCharTok{\textgreater{}} \FloatTok{0.5}\NormalTok{] }\OtherTok{\textless{}{-}} \StringTok{"Yes"}
\FunctionTok{table}\NormalTok{(boston.pred, boston[boston.test,]}\SpecialCharTok{$}\NormalTok{crim\_above\_med)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##            
## boston.pred No Yes
##         No  36  12
##         Yes  5  49
\end{verbatim}

The above table shows that the model correctly predicted \texttt{crim\_above\_med} from \texttt{nox} 85 times out of 102, with 5 false positives and 12 false negatives. The accuracy of the model can be computed via the \texttt{mean()} of how often the predictions match the test data, a neat trick:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{mean}\NormalTok{(boston.pred }\SpecialCharTok{==}\NormalTok{ boston[boston.test,]}\SpecialCharTok{$}\NormalTok{crim\_above\_med)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.8333333
\end{verbatim}

The regression can be plotted with \texttt{ggplot2} and the \texttt{stat\_smooth()} function, utilizing the \texttt{glm} method, like so:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(boston, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ nox, }\AttributeTok{y =}\NormalTok{ crim\_above\_med\_num)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{alpha =}\NormalTok{ .}\DecValTok{5}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{stat\_smooth}\NormalTok{(}
    \AttributeTok{method =} \StringTok{"glm"}\NormalTok{,}
    \AttributeTok{formula =}\NormalTok{ y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x,}
    \AttributeTok{se =} \ConstantTok{FALSE}\NormalTok{,}
    \AttributeTok{method.args =} \FunctionTok{list}\NormalTok{(}\AttributeTok{family =}\NormalTok{ binomial),}
    \AttributeTok{col =} \StringTok{"red"}\NormalTok{,}
    \AttributeTok{lty =} \DecValTok{2}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\includegraphics{_main_files/figure-latex/unnamed-chunk-16-1.pdf}

This shows that an observation with a \texttt{nox} value of 0.6 would have around a 90\% probability of \texttt{crim\_above\_med} being true, or 1, while an observation with a \texttt{nox} value of 0.5 would only have around a 25\% probability of \texttt{crim\_above\_med} being true.

\hypertarget{where-to-learn-more-2}{%
\section{Where to learn more}\label{where-to-learn-more-2}}

\begin{itemize}
\tightlist
\item
  Chapter 4 - 4.4 in \citet{ISLR}
\item
  \href{https://www.youtube.com/watch?v=yIYKR4sgzI8}{StatQuest: Logistic Regression}
\end{itemize}

\hypertarget{notes-1}{%
\section{Notes}\label{notes-1}}

Like with linear regression, the ``multiple'' flavor of logistic regression works much the same way, with the specification of additional predictors in the formula, and so doesn't really warrant a whole chapter of its own. The selection of variables requires some effort, and is addressed under \protect\hyperlink{best_subset_selection}{best subser selection}.

\hypertarget{linear-discriminant-analysis}{%
\chapter{Linear Discriminant Analysis (LDA)}\label{linear-discriminant-analysis}}

\hypertarget{tldr-3}{%
\section{TL;DR}\label{tldr-3}}

\begin{description}
\tightlist
\item[What it does]
Separates observations into categories, similar \protect\hyperlink{logistic-regression}{logistic regression}, but creating actual groups (separated by a line) rather than per-observation probabilities
\item[When to do it]
When exploring various classifiers to see which works best for a given data set
\item[How to do it]
With the \texttt{lda()} function from the \href{https://cran.r-project.org/web/packages/MASS/index.html}{MASS} library, using training and testing sets
\item[How to assess it]
Assess the accuracy of the predictions on the test set after training
\end{description}

\hypertarget{what-it-does-3}{%
\section{What it does}\label{what-it-does-3}}

Similar to \protect\hyperlink{principal-component-analysis}{principal component analysys} (PCA), LDA reduces dimensionality and finds the best single axis to separate two (or more) groups of observations into categories using a least-squares method of distance from a mean. Like PCA, it returns a set of new axes for the data, organized by importance, so that the first axis is the one that explains the largest amount of variance, and so on, down to \(p - 1\) axes where \(p\) is the number of categories/dimensions.

So, LDA will return 3 axes for a set of observations with 4 categories, or 1 for a set of observations with 2 categories. Each axis will have a loading score that indicates which variables had the biggest impacts on it.

\hypertarget{when-to-do-it-3}{%
\section{When to do it}\label{when-to-do-it-3}}

When you want to see if it will work better than other classification methods! It should always be tried along with other classifiers like \protect\hyperlink{logistic-regression}{logistic regression}, \protect\hyperlink{quadratic-discriminant-analysis}{quadratic discriminant analysis}, and \protect\hyperlink{naive-bayes}{Naive Bayes}.

\hypertarget{how-to-do-it-3}{%
\section{How to do it}\label{how-to-do-it-3}}

Again, using the Boston data from the \protect\hyperlink{logistic-regression}{logistic regression} chapter:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data}\NormalTok{(Boston)}
\NormalTok{boston }\OtherTok{\textless{}{-}}\NormalTok{ Boston }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}
    \CommentTok{\# Create the categorical crim\_above\_med response variable}
    \AttributeTok{crim\_above\_med =} \FunctionTok{as.factor}\NormalTok{(}\FunctionTok{ifelse}\NormalTok{(crim }\SpecialCharTok{\textgreater{}} \FunctionTok{median}\NormalTok{(crim), }\StringTok{"Yes"}\NormalTok{, }\StringTok{"No"}\NormalTok{)),}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

We again split the data into training and test sets:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1235}\NormalTok{)}
\NormalTok{boston.training }\OtherTok{\textless{}{-}} \FunctionTok{rep}\NormalTok{(}\ConstantTok{FALSE}\NormalTok{, }\FunctionTok{nrow}\NormalTok{(boston))}
\NormalTok{boston.training[}\FunctionTok{sample}\NormalTok{(}\FunctionTok{nrow}\NormalTok{(boston), }\FunctionTok{nrow}\NormalTok{(boston) }\SpecialCharTok{*} \FloatTok{0.8}\NormalTok{)] }\OtherTok{\textless{}{-}} \ConstantTok{TRUE}
\NormalTok{boston.test }\OtherTok{\textless{}{-}} \SpecialCharTok{!}\NormalTok{boston.training}
\end{Highlighting}
\end{Shaded}

And fit the above model to the training data using the \texttt{lda()} function from the \href{https://cran.r-project.org/package=MASS}{MASS} library\footnote{Note to my fellow New Englanders: the MASS library is named for a book titled ``Modern Applied Statistics with S'', and although it does contain an earlier version of the Boston dataset (among many others), it has nothing to do with Massachusetts, and our Boston dataset comes from the ISLR2 library.} (note: there is no \texttt{family} argument as with \texttt{glm()} in \protect\hyperlink{logistic-regression}{logistic regression}, but the calls are otherwise identical):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{boston\_lda.fits }\OtherTok{\textless{}{-}}
  \FunctionTok{lda}\NormalTok{(}
\NormalTok{    crim\_above\_med }\SpecialCharTok{\textasciitilde{}}\NormalTok{ nox,}
    \AttributeTok{data =}\NormalTok{ boston,}
    \AttributeTok{subset =}\NormalTok{ boston.training,}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\hypertarget{how-to-assess-it-3}{%
\section{How to assess it}\label{how-to-assess-it-3}}

The fit can be directly examined:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{boston\_lda.fits}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Call:
## lda(crim_above_med ~ nox, data = boston, , subset = boston.training)
## 
## Prior probabilities of groups:
##        No       Yes 
## 0.5247525 0.4752475 
## 
## Group means:
##           nox
## No  0.4710519
## Yes 0.6357865
## 
## Coefficients of linear discriminants:
##          LD1
## nox 12.29052
\end{verbatim}

In the above output, the \texttt{Prior\ probabilities\ of\ groups} information is just the percentage of observations in each of the outcome categories:

\begin{Shaded}
\begin{Highlighting}[]
\SpecialCharTok{\textgreater{}} \FunctionTok{mean}\NormalTok{(boston[boston.training,]}\SpecialCharTok{$}\NormalTok{crim\_above\_med }\SpecialCharTok{==} \StringTok{"Yes"}\NormalTok{)}
\NormalTok{[}\DecValTok{1}\NormalTok{] }\FloatTok{0.4752475}
\SpecialCharTok{\textgreater{}} \FunctionTok{mean}\NormalTok{(boston[boston.training,]}\SpecialCharTok{$}\NormalTok{crim\_above\_med }\SpecialCharTok{==} \StringTok{"No"}\NormalTok{)}
\NormalTok{[}\DecValTok{1}\NormalTok{] }\FloatTok{0.5247525}
\end{Highlighting}
\end{Shaded}

The \texttt{Group\ means} section shows the means of the predictor variable (\texttt{nox} in this case) for each of the categories:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{boston[boston.training, ] }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{(crim\_above\_med) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarize}\NormalTok{(}\StringTok{"Group means"} \OtherTok{=} \FunctionTok{mean}\NormalTok{(nox))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 2 x 2
##   crim_above_med `Group means`
##   <fct>                  <dbl>
## 1 No                     0.471
## 2 Yes                    0.636
\end{verbatim}

And the ``Coefficient'' is the linear coefficient for the predictor variable \texttt{nox}, which determines the slope of the line that separates the categories.

The fit can be plotted:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(boston\_lda.fits)}
\end{Highlighting}
\end{Shaded}

\includegraphics{_main_files/figure-latex/unnamed-chunk-23-1.pdf}

The fit should be used to predict the test data, and the model can be assessed on its results:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{boston\_lda.pred }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(boston\_lda.fits, boston[boston.test, ])}
\FunctionTok{table}\NormalTok{(boston\_lda.pred}\SpecialCharTok{$}\NormalTok{class, boston[boston.test, ]}\SpecialCharTok{$}\NormalTok{crim\_above\_med)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      
##       No Yes
##   No  38  12
##   Yes  3  49
\end{verbatim}

In this example, LDA made the correct categorization 87 times out of 102, with 3 false positives and 12 false negatives. Again, we can compute the prediction accuracy by the mean of the correct-to-wrong guesses:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{mean}\NormalTok{(boston\_lda.pred}\SpecialCharTok{$}\NormalTok{class }\SpecialCharTok{==}\NormalTok{ boston[boston.test, ]}\SpecialCharTok{$}\NormalTok{crim\_above\_med)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.8529412
\end{verbatim}

If the accuracy rate is good enough for your purposes, you can use the fit in the same manner to make predictions for new data.

\hypertarget{where-to-learn-more-3}{%
\section{Where to learn more}\label{where-to-learn-more-3}}

\begin{itemize}
\tightlist
\item
  Chapter 4.4.1 - 4.4.2 in \citet{ISLR}
\item
  \href{https://www.youtube.com/watch?v=azXCzI57Yfc}{StatQuest: Linear Discriminant Analysis}
\end{itemize}

\hypertarget{quadratic-discriminant-analysis}{%
\chapter{Quadratic Discriminant Analysis}\label{quadratic-discriminant-analysis}}

\hypertarget{tldr-4}{%
\section{TL;DR}\label{tldr-4}}

\begin{description}
\tightlist
\item[What it does]
Assigns observations to categories like \protect\hyperlink{linear-discriminant-analysis}{linear discriminant analysis}, but with a more flexible / curved boundary rather than a straight line
\item[When to do it]
When \protect\hyperlink{linear-discriminant-analysis}{linear discriminant analysis} isn't giving good enough results (esp.~if the dataset is very large)
\item[How to do it]
With the \texttt{qda()} function from the \href{https://cran.r-project.org/package=MASS}{MASS} library, using training and testing sets
\item[How to assess it]
Assess the accuracy of the predictions on the test set after training
\end{description}

\hypertarget{what-it-does-4}{%
\section{What it does}\label{what-it-does-4}}

Just like \protect\hyperlink{linear-discriminant-analysis}{linear discriminant analysis}, quadratic discriminant analysis attempts to separate observations into two or more classes or categories, but it allows for a curved boundary between the classes. Which approach gives better results depends on the shape of the Bayes decision boundary for any particular dataset. The following figure from \citet{ISLR} shows the two approaches:



\begin{figure}
\centering
\includegraphics{images/4_9.png}
\caption{\label{fig:img-slr49}LDA and QDA plots (source: \citet{ISLR}, p.~154)}
\end{figure}

The purple dashed line in both diagrams shows the Bayes decision boundary. The LDA line in the left plot shows this class separation line as straight; the QDA line on the right is curved.

\hypertarget{when-to-do-it-4}{%
\section{When to do it}\label{when-to-do-it-4}}

Use QDA when the linear separation of \protect\hyperlink{linear-discriminant-analysis}{linear discriminant analysis} isn't giving good enough results, and you think a curved (or quadratic) line might fit better.

\hypertarget{how-to-do-it-4}{%
\section{How to do it}\label{how-to-do-it-4}}

Exactly like \protect\hyperlink{linear-discriminant-analysis}{linear discriminant analysis}, except for the \texttt{qda()} call instead of \texttt{lda()}. Train the model on training data, assess it against test data.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data}\NormalTok{(Boston)}
\NormalTok{boston }\OtherTok{\textless{}{-}}\NormalTok{ Boston }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}
    \CommentTok{\# Create the categorical crim\_above\_med response variable}
    \AttributeTok{crim\_above\_med =} \FunctionTok{as.factor}\NormalTok{(}\FunctionTok{ifelse}\NormalTok{(crim }\SpecialCharTok{\textgreater{}} \FunctionTok{median}\NormalTok{(crim), }\StringTok{"Yes"}\NormalTok{, }\StringTok{"No"}\NormalTok{)),}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

We again split the data into training and test sets:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1235}\NormalTok{)}
\NormalTok{boston.training }\OtherTok{\textless{}{-}} \FunctionTok{rep}\NormalTok{(}\ConstantTok{FALSE}\NormalTok{, }\FunctionTok{nrow}\NormalTok{(boston))}
\NormalTok{boston.training[}\FunctionTok{sample}\NormalTok{(}\FunctionTok{nrow}\NormalTok{(boston), }\FunctionTok{nrow}\NormalTok{(boston) }\SpecialCharTok{*} \FloatTok{0.8}\NormalTok{)] }\OtherTok{\textless{}{-}} \ConstantTok{TRUE}
\NormalTok{boston.test }\OtherTok{\textless{}{-}} \SpecialCharTok{!}\NormalTok{boston.training}
\end{Highlighting}
\end{Shaded}

and call \texttt{qda()} on the training data:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{boston\_qda.fits }\OtherTok{\textless{}{-}}
  \FunctionTok{qda}\NormalTok{(}
\NormalTok{    crim\_above\_med }\SpecialCharTok{\textasciitilde{}}\NormalTok{ nox,}
    \AttributeTok{data =}\NormalTok{ boston,}
    \AttributeTok{subset =}\NormalTok{ boston.training,}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\hypertarget{how-to-assess-it-4}{%
\section{How to assess it}\label{how-to-assess-it-4}}

As with \texttt{lda()}, the resulting fit can be directly examined:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{boston\_qda.fits}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Call:
## qda(crim_above_med ~ nox, data = boston, , subset = boston.training)
## 
## Prior probabilities of groups:
##        No       Yes 
## 0.5247525 0.4752475 
## 
## Group means:
##           nox
## No  0.4710519
## Yes 0.6357865
\end{verbatim}

Note that in this example, the \texttt{Prior\ probabilities\ of\ groups} and \texttt{Group\ means} assessments are identical to those from \texttt{lda()}. There is no linear discriminant coefficient, because this is not a linear discriminant! The rest of the output is identical.

We then call \texttt{predict} on the QDA fit, to predict the \texttt{crim\_above\_med} variable from \texttt{nox}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{boston\_qda.pred }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(boston\_qda.fits, boston[boston.test, ])}
\FunctionTok{table}\NormalTok{(boston\_qda.pred}\SpecialCharTok{$}\NormalTok{class, boston[boston.test, ]}\SpecialCharTok{$}\NormalTok{crim\_above\_med)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      
##       No Yes
##   No  38  12
##   Yes  3  49
\end{verbatim}

In this example, just like LDA, QDA made the correct categorization 87 times out of 102, with 3 false positives and 12 false negatives. And again, we can compute the prediction accuracy by the mean of the correct-to-wrong guesses:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{mean}\NormalTok{(boston\_qda.pred}\SpecialCharTok{$}\NormalTok{class }\SpecialCharTok{==}\NormalTok{ boston[boston.test, ]}\SpecialCharTok{$}\NormalTok{crim\_above\_med)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.8529412
\end{verbatim}

In this case, QDA offered no increase in prediction accuracy over LDA, so there is no reason to prefer it (although it also performed no poorer).

\hypertarget{where-to-learn-more-4}{%
\section{Where to learn more}\label{where-to-learn-more-4}}

\begin{itemize}
\tightlist
\item
  Chapter 4.4.3 in \citet{ISLR}
\end{itemize}

\hypertarget{naive-bayes}{%
\chapter{Naive Bayes}\label{naive-bayes}}

\hypertarget{tldr-5}{%
\section{TL;DR}\label{tldr-5}}

\begin{description}
\tightlist
\item[What it does]
Provides yet another way to try to classify observations into one or more categories
\item[When to do it]
When you're rounding out your exhaustive try-them-all process to try to determine the best classifier for your data after \protect\hyperlink{linear-discriminant-analysis}{linear discriminant analysis} and \protect\hyperlink{quadratic-discriminant-analysis}{quadratic discriminant analysis}
\item[How to do it]
With the \texttt{naiveBayes()} function from the \href{https://cran.r-project.org/package=e1071}{e1071} library
\item[How to assess it]
As with the other methods, assess it on test data after training and see if its accuracy is better than other methods
\end{description}

\hypertarget{what-it-does-5}{%
\section{What it does}\label{what-it-does-5}}

Naive Bayes makes the assumption that there is no association between the predictors. According to \citet{ISLR}, this still yields ``decent results'' even though that's generally not true, especially in datasets where the number of observations \(n\) is smallish relative to the number of variables \(p\).

\hypertarget{when-to-do-it-5}{%
\section{When to do it}\label{when-to-do-it-5}}

When you're rounding our your assessment of the accuracy of the various classification methods looking for the best one.

TODO: there must be a better explanation for this.

\hypertarget{how-to-do-it-5}{%
\section{How to do it}\label{how-to-do-it-5}}

Exactly like \protect\hyperlink{linear-discriminant-analysis}{linear discriminant analysis} and \protect\hyperlink{quadratic-discriminant-analysis}{quadratic discriminant analysis}, except for the \texttt{naiveBayes()} call instead of \texttt{qda()} or \texttt{lda()}. Train the model on training data, assess it against test data.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data}\NormalTok{(Boston)}
\NormalTok{boston }\OtherTok{\textless{}{-}}\NormalTok{ Boston }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}
    \CommentTok{\# Create the categorical crim\_above\_med response variable}
    \AttributeTok{crim\_above\_med =} \FunctionTok{as.factor}\NormalTok{(}\FunctionTok{ifelse}\NormalTok{(crim }\SpecialCharTok{\textgreater{}} \FunctionTok{median}\NormalTok{(crim), }\StringTok{"Yes"}\NormalTok{, }\StringTok{"No"}\NormalTok{)),}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

We again split the data into training and test sets:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1235}\NormalTok{)}
\NormalTok{boston.training }\OtherTok{\textless{}{-}} \FunctionTok{rep}\NormalTok{(}\ConstantTok{FALSE}\NormalTok{, }\FunctionTok{nrow}\NormalTok{(boston))}
\NormalTok{boston.training[}\FunctionTok{sample}\NormalTok{(}\FunctionTok{nrow}\NormalTok{(boston), }\FunctionTok{nrow}\NormalTok{(boston) }\SpecialCharTok{*} \FloatTok{0.8}\NormalTok{)] }\OtherTok{\textless{}{-}} \ConstantTok{TRUE}
\NormalTok{boston.test }\OtherTok{\textless{}{-}} \SpecialCharTok{!}\NormalTok{boston.training}
\end{Highlighting}
\end{Shaded}

and call \texttt{naiveBayes()} on the training data:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{boston\_nb.fit }\OtherTok{\textless{}{-}}
  \FunctionTok{naiveBayes}\NormalTok{(}
\NormalTok{    crim\_above\_med }\SpecialCharTok{\textasciitilde{}}\NormalTok{ nox,}
    \AttributeTok{data =}\NormalTok{ boston,}
    \AttributeTok{subset =}\NormalTok{ boston.training,}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\hypertarget{how-to-assess-it-5}{%
\section{How to assess it}\label{how-to-assess-it-5}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{boston\_nb.fit}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Naive Bayes Classifier for Discrete Predictors
## 
## Call:
## naiveBayes.default(x = X, y = Y, laplace = laplace)
## 
## A-priori probabilities:
## Y
##        No       Yes 
## 0.5247525 0.4752475 
## 
## Conditional probabilities:
##      nox
## Y          [,1]       [,2]
##   No  0.4710519 0.05635737
##   Yes 0.6357865 0.10210051
\end{verbatim}

In the output above, we see our \texttt{A-priori\ probabilities}, which like the \texttt{Prior\ probabilities} of \texttt{lda()} and \texttt{qda()} show the percentage of observations in each of the outcome categories:

\begin{Shaded}
\begin{Highlighting}[]
\SpecialCharTok{\textgreater{}} \FunctionTok{mean}\NormalTok{(boston[boston.training,]}\SpecialCharTok{$}\NormalTok{crim\_above\_med }\SpecialCharTok{==} \StringTok{"Yes"}\NormalTok{)}
\NormalTok{[}\DecValTok{1}\NormalTok{] }\FloatTok{0.4752475}
\SpecialCharTok{\textgreater{}} \FunctionTok{mean}\NormalTok{(boston[boston.training,]}\SpecialCharTok{$}\NormalTok{crim\_above\_med }\SpecialCharTok{==} \StringTok{"No"}\NormalTok{)}
\NormalTok{[}\DecValTok{1}\NormalTok{] }\FloatTok{0.5247525}
\end{Highlighting}
\end{Shaded}

The \texttt{Conditional\ probabilities} are identical to the \texttt{Group\ means} from \texttt{lda()} and \texttt{qda()}, showing the means of the predictor variable (\texttt{nox} in this case) for each of the categories:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{boston[boston.training, ] }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{(crim\_above\_med) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarize}\NormalTok{(}\StringTok{"Group means"} \OtherTok{=} \FunctionTok{mean}\NormalTok{(nox))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 2 x 2
##   crim_above_med `Group means`
##   <fct>                  <dbl>
## 1 No                     0.471
## 2 Yes                    0.636
\end{verbatim}

All of these results are the same as \texttt{lda()} and \texttt{qda()}. The Naive Bayes output also includes a second column in the \texttt{Conditional\ probabilities} section, which is the standard deviation for the (in this case \texttt{nox}) variable in each category.

After the fit is built, we can run prediction as with the other methods, to predict the \texttt{crim\_above\_med} variable from \texttt{nox} (note that we just make a table of the prediction output itself, without the \texttt{\$class} attribute):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{boston\_nb.pred }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(boston\_nb.fit, boston[boston.test, ])}
\FunctionTok{table}\NormalTok{(boston\_nb.pred, boston[boston.test, ]}\SpecialCharTok{$}\NormalTok{crim\_above\_med)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##               
## boston_nb.pred No Yes
##            No  38  12
##            Yes  3  49
\end{verbatim}

In this example, just like LDA and QDA, Naive Bayes made the correct categorization 87 times out of 102, with 3 false positives and 12 false negatives. And again, we can compute the prediction accuracy by the mean of the correct-to-wrong guesses:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{mean}\NormalTok{(boston\_nb.pred }\SpecialCharTok{==}\NormalTok{ boston[boston.test, ]}\SpecialCharTok{$}\NormalTok{crim\_above\_med)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.8529412
\end{verbatim}

We see again here that Naive Bayes performed identically to \texttt{lda()} and \texttt{qda()}. This means we probably need a better example for this document.

\hypertarget{where-to-learn-more-5}{%
\section{Where to learn more}\label{where-to-learn-more-5}}

\begin{itemize}
\tightlist
\item
  Chapter 4.4.4 in \citet{ISLR}
\end{itemize}

\hypertarget{k-nearest-neighbors}{%
\chapter{K-Nearest Neighbors}\label{k-nearest-neighbors}}

\hypertarget{tldr-6}{%
\section{TL;DR}\label{tldr-6}}

\begin{description}
\tightlist
\item[What it does]
K-Nearest Neighbors classifies observations by judging their proximity to (\(k\)) other nearby classified neighbors.
\item[When to do it]
When you're working through all of the various classification methods to see which one works the best for your data
\item[How to do it]
With the \texttt{knn()} function from the \href{https://cran.r-project.org/package=class}{class} library
\item[How to assess it]
Check the prediction accuracy, as with the other classification methods
\end{description}

\hypertarget{what-it-does-6}{%
\section{What it does}\label{what-it-does-6}}

K-Nearest Neighbors starts with known categories of clustered data, and assigns new data to one of these categories by grouping it with its (\(k\)) nearest neighbor(s). This is judged in whatever dimensional space is appropriate to the number of predictor variables; if only using one predictor, as in the example here, then the points are simply arranged on a line; 2 predictors puts them on a plane, and so forth.

Once the new observation is located against the known categories, a sort of ``lasso'' extends out from the point in all available directions until it contains \(k\) other neighbors. Whichever category has the most points is assigned to the new observation.

\hypertarget{when-to-do-it-6}{%
\section{When to do it}\label{when-to-do-it-6}}

When you're still looking for the best classification method for your data. Maybe this will be it this time!

\hypertarget{how-to-do-it-6}{%
\section{How to do it}\label{how-to-do-it-6}}

We will continue with the Boston data set and the same training split.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data}\NormalTok{(Boston)}
\NormalTok{boston }\OtherTok{\textless{}{-}}\NormalTok{ Boston }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}
    \CommentTok{\# Create the categorical crim\_above\_med response variable}
    \AttributeTok{crim\_above\_med =} \FunctionTok{as.factor}\NormalTok{(}\FunctionTok{ifelse}\NormalTok{(crim }\SpecialCharTok{\textgreater{}} \FunctionTok{median}\NormalTok{(crim), }\StringTok{"Yes"}\NormalTok{, }\StringTok{"No"}\NormalTok{)),}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

We again split the data into training and test sets:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1235}\NormalTok{)}
\NormalTok{boston.training }\OtherTok{\textless{}{-}} \FunctionTok{rep}\NormalTok{(}\ConstantTok{FALSE}\NormalTok{, }\FunctionTok{nrow}\NormalTok{(boston))}
\NormalTok{boston.training[}\FunctionTok{sample}\NormalTok{(}\FunctionTok{nrow}\NormalTok{(boston), }\FunctionTok{nrow}\NormalTok{(boston) }\SpecialCharTok{*} \FloatTok{0.8}\NormalTok{)] }\OtherTok{\textless{}{-}} \ConstantTok{TRUE}
\NormalTok{boston.test }\OtherTok{\textless{}{-}} \SpecialCharTok{!}\NormalTok{boston.training}
\end{Highlighting}
\end{Shaded}

Unlike the other classification methods used so far, the \texttt{knn()} function needs the data columns in a simple matrix. Also, we supply the training and test data together in a single call to \texttt{knn()}.

So first, create the matrices from the data columns. We only have one column in our model here, so we need to convert it to a \texttt{data.frame} because \texttt{knn()} won't accept a plain vector:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{train.X }\OtherTok{\textless{}{-}} \FunctionTok{as.data.frame}\NormalTok{(boston[boston.training, ]}\SpecialCharTok{$}\NormalTok{nox)}
\NormalTok{test.X }\OtherTok{\textless{}{-}} \FunctionTok{as.data.frame}\NormalTok{(boston[boston.test, ]}\SpecialCharTok{$}\NormalTok{nox)}
\end{Highlighting}
\end{Shaded}

If we had more than one column, we could instead bind them with \texttt{cbind()}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{train.X }\OtherTok{\textless{}{-}} \FunctionTok{cbind}\NormalTok{(boston}\SpecialCharTok{$}\NormalTok{nox, boston}\SpecialCharTok{$}\NormalTok{something\_else)[boston.training, ]}
\NormalTok{test.X }\OtherTok{\textless{}{-}} \FunctionTok{cbind}\NormalTok{(boston}\SpecialCharTok{$}\NormalTok{nox, boston}\SpecialCharTok{$}\NormalTok{something\_else)[boston.test, ]}
\end{Highlighting}
\end{Shaded}

Note that it is highly recommended to standardize all columns to the same scale when using more than one, unless a weighted scale is desired, and even then it would likely be better to standardize first and then scale from there.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{train.X }\OtherTok{\textless{}{-}} \FunctionTok{cbind}\NormalTok{(}\FunctionTok{scale}\NormalTok{(boston}\SpecialCharTok{$}\NormalTok{nox), }\FunctionTok{scale}\NormalTok{(boston}\SpecialCharTok{$}\NormalTok{something\_else))[boston.training, ]}
\NormalTok{test.X }\OtherTok{\textless{}{-}} \FunctionTok{cbind}\NormalTok{(}\FunctionTok{scale}\NormalTok{(boston}\SpecialCharTok{$}\NormalTok{nox), }\FunctionTok{scale}\NormalTok{(boston}\SpecialCharTok{$}\NormalTok{something\_else))[boston.test, ]}
\end{Highlighting}
\end{Shaded}

We also extract the outcome variable into a vector.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Also extract the outcome variable.}
\NormalTok{train.crim\_above\_med }\OtherTok{\textless{}{-}}\NormalTok{ boston}\SpecialCharTok{$}\NormalTok{crim\_above\_med[boston.training]}
\end{Highlighting}
\end{Shaded}

Now set a seed and run the \texttt{knn()} function. Select a value for \(k\), the number of nearest neighbors to use in the classifier; here we will start with 1, which means each new observation will be classified according to its single closest neighbor. (Higher values will decide based on the majority of the \(k\) nearest neighbors; keeping \(k\) prime, if more than 1, will avoid ties; alternately, ties can be randomly assigned.)

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1235}\NormalTok{)}
\NormalTok{knn.pred }\OtherTok{\textless{}{-}} \FunctionTok{knn}\NormalTok{(train.X, test.X, train.crim\_above\_med, }\AttributeTok{k =} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{how-to-assess-it-6}{%
\section{How to assess it}\label{how-to-assess-it-6}}

Check the table of predictions against the test data and see how it did.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{table}\NormalTok{(knn.pred, boston[boston.test,]}\SpecialCharTok{$}\NormalTok{crim\_above\_med)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##         
## knn.pred No Yes
##      No  40   2
##      Yes  1  59
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{mean}\NormalTok{(knn.pred }\SpecialCharTok{==}\NormalTok{ boston[boston.test,]}\SpecialCharTok{$}\NormalTok{crim\_above\_med)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.9705882
\end{verbatim}

In this case, we achieved 97\% accuracy with \texttt{knn()} and \(k\) = 1. If we didn't get a good enough result, we could run \texttt{knn()} again with different values for \(k\) to see if we could improve the results. The best result here can then be compared against the results with other classification methods.

(It's unusual for \(k\) = 1 to be the best number, as it will tend to overfit; some exploration will be needed to find the best results, and it's simple enough to try a range in a loop.)

\hypertarget{where-to-learn-more-6}{%
\section{Where to learn more}\label{where-to-learn-more-6}}

\begin{itemize}
\tightlist
\item
  Chapter 4.7.6 (lab) in \citet{ISLR}
\item
  \href{https://www.youtube.com/watch?v=HVXime0nQeI}{StatQuest: K nearest neighbors}
\end{itemize}

\hypertarget{poisson-regression}{%
\chapter{TODO: Poisson Regression}\label{poisson-regression}}

\hypertarget{tldr-7}{%
\section{TL;DR}\label{tldr-7}}

\begin{description}
\tightlist
\item[What it does]
TODO
\item[When to do it]
TODO
\item[How to do it]
TODO
\item[How to assess it]
TODO
\end{description}

\hypertarget{what-it-does-7}{%
\section{What it does}\label{what-it-does-7}}

TODO

\hypertarget{when-to-do-it-7}{%
\section{When to do it}\label{when-to-do-it-7}}

TODO

\hypertarget{how-to-do-it-7}{%
\section{How to do it}\label{how-to-do-it-7}}

TODO

\hypertarget{how-to-assess-it-7}{%
\section{How to assess it}\label{how-to-assess-it-7}}

TODO

\hypertarget{where-to-learn-more-7}{%
\section{Where to learn more}\label{where-to-learn-more-7}}

TODO

\hypertarget{cross-validation}{%
\chapter{TODO: Cross-Validation}\label{cross-validation}}

\hypertarget{tldr-8}{%
\section{TL;DR}\label{tldr-8}}

\begin{description}
\tightlist
\item[What it does]
TODO
\item[When to do it]
TODO
\item[How to do it]
TODO
\item[How to assess it]
TODO
\end{description}

\hypertarget{what-it-does-8}{%
\section{What it does}\label{what-it-does-8}}

TODO

\hypertarget{when-to-do-it-8}{%
\section{When to do it}\label{when-to-do-it-8}}

TODO

\hypertarget{how-to-do-it-8}{%
\section{How to do it}\label{how-to-do-it-8}}

TODO

\hypertarget{how-to-assess-it-8}{%
\section{How to assess it}\label{how-to-assess-it-8}}

TODO

\hypertarget{where-to-learn-more-8}{%
\section{Where to learn more}\label{where-to-learn-more-8}}

TODO

\hypertarget{bootstrap}{%
\chapter{TODO: Bootstrap}\label{bootstrap}}

\hypertarget{tldr-9}{%
\section{TL;DR}\label{tldr-9}}

\begin{description}
\tightlist
\item[What it does]
TODO
\item[When to do it]
TODO
\item[How to do it]
TODO
\item[How to assess it]
TODO
\end{description}

\hypertarget{what-it-does-9}{%
\section{What it does}\label{what-it-does-9}}

TODO

\hypertarget{when-to-do-it-9}{%
\section{When to do it}\label{when-to-do-it-9}}

TODO

\hypertarget{how-to-do-it-9}{%
\section{How to do it}\label{how-to-do-it-9}}

TODO

\hypertarget{how-to-assess-it-9}{%
\section{How to assess it}\label{how-to-assess-it-9}}

TODO

\hypertarget{where-to-learn-more-9}{%
\section{Where to learn more}\label{where-to-learn-more-9}}

TODO

\hypertarget{best-subset-selection}{%
\chapter{TODO: Best Subset Selection}\label{best-subset-selection}}

\hypertarget{tldr-10}{%
\section{TL;DR}\label{tldr-10}}

\begin{description}
\tightlist
\item[What it does]
TODO
\item[When to do it]
TODO
\item[How to do it]
TODO
\item[How to assess it]
TODO
\end{description}

\hypertarget{what-it-does-10}{%
\section{What it does}\label{what-it-does-10}}

TODO

\hypertarget{when-to-do-it-10}{%
\section{When to do it}\label{when-to-do-it-10}}

TODO

\hypertarget{how-to-do-it-10}{%
\section{How to do it}\label{how-to-do-it-10}}

TODO

\hypertarget{how-to-assess-it-10}{%
\section{How to assess it}\label{how-to-assess-it-10}}

TODO

\hypertarget{where-to-learn-more-10}{%
\section{Where to learn more}\label{where-to-learn-more-10}}

TODO

\hypertarget{stepwise-selection}{%
\chapter{TODO: Stepwise Selection}\label{stepwise-selection}}

\hypertarget{tldr-11}{%
\section{TL;DR}\label{tldr-11}}

\begin{description}
\tightlist
\item[What it does]
TODO
\item[When to do it]
TODO
\item[How to do it]
TODO
\item[How to assess it]
TODO
\end{description}

\hypertarget{what-it-does-11}{%
\section{What it does}\label{what-it-does-11}}

TODO

\hypertarget{when-to-do-it-11}{%
\section{When to do it}\label{when-to-do-it-11}}

TODO

\hypertarget{how-to-do-it-11}{%
\section{How to do it}\label{how-to-do-it-11}}

TODO

\hypertarget{how-to-assess-it-11}{%
\section{How to assess it}\label{how-to-assess-it-11}}

TODO

\hypertarget{where-to-learn-more-11}{%
\section{Where to learn more}\label{where-to-learn-more-11}}

TODO

\hypertarget{ridge-regression}{%
\chapter{TODO: Ridge Regression}\label{ridge-regression}}

\hypertarget{tldr-12}{%
\section{TL;DR}\label{tldr-12}}

\begin{description}
\tightlist
\item[What it does]
TODO
\item[When to do it]
TODO
\item[How to do it]
TODO
\item[How to assess it]
TODO
\end{description}

\hypertarget{what-it-does-12}{%
\section{What it does}\label{what-it-does-12}}

TODO

\hypertarget{when-to-do-it-12}{%
\section{When to do it}\label{when-to-do-it-12}}

TODO

\hypertarget{how-to-do-it-12}{%
\section{How to do it}\label{how-to-do-it-12}}

TODO

\hypertarget{how-to-assess-it-12}{%
\section{How to assess it}\label{how-to-assess-it-12}}

TODO

\hypertarget{where-to-learn-more-12}{%
\section{Where to learn more}\label{where-to-learn-more-12}}

TODO

\hypertarget{lasso}{%
\chapter{TODO: Lasso}\label{lasso}}

\hypertarget{tldr-13}{%
\section{TL;DR}\label{tldr-13}}

\begin{description}
\tightlist
\item[What it does]
TODO
\item[When to do it]
TODO
\item[How to do it]
TODO
\item[How to assess it]
TODO
\end{description}

\hypertarget{what-it-does-13}{%
\section{What it does}\label{what-it-does-13}}

TODO

\hypertarget{when-to-do-it-13}{%
\section{When to do it}\label{when-to-do-it-13}}

TODO

\hypertarget{how-to-do-it-13}{%
\section{How to do it}\label{how-to-do-it-13}}

TODO

\hypertarget{how-to-assess-it-13}{%
\section{How to assess it}\label{how-to-assess-it-13}}

TODO

\hypertarget{where-to-learn-more-13}{%
\section{Where to learn more}\label{where-to-learn-more-13}}

TODO

\hypertarget{principal-component-regression}{%
\chapter{TODO: Principal Component Regression}\label{principal-component-regression}}

\hypertarget{tldr-14}{%
\section{TL;DR}\label{tldr-14}}

\begin{description}
\tightlist
\item[What it does]
TODO
\item[When to do it]
TODO
\item[How to do it]
TODO
\item[How to assess it]
TODO
\end{description}

\hypertarget{what-it-does-14}{%
\section{What it does}\label{what-it-does-14}}

TODO

\hypertarget{when-to-do-it-14}{%
\section{When to do it}\label{when-to-do-it-14}}

TODO

\hypertarget{how-to-do-it-14}{%
\section{How to do it}\label{how-to-do-it-14}}

TODO

\hypertarget{how-to-assess-it-14}{%
\section{How to assess it}\label{how-to-assess-it-14}}

TODO

\hypertarget{where-to-learn-more-14}{%
\section{Where to learn more}\label{where-to-learn-more-14}}

TODO

\hypertarget{bagging}{%
\chapter{TODO: Bagging}\label{bagging}}

\hypertarget{tldr-15}{%
\section{TL;DR}\label{tldr-15}}

\begin{description}
\tightlist
\item[What it does]
TODO
\item[When to do it]
TODO
\item[How to do it]
TODO
\item[How to assess it]
TODO
\end{description}

\hypertarget{what-it-does-15}{%
\section{What it does}\label{what-it-does-15}}

TODO

\hypertarget{when-to-do-it-15}{%
\section{When to do it}\label{when-to-do-it-15}}

TODO

\hypertarget{how-to-do-it-15}{%
\section{How to do it}\label{how-to-do-it-15}}

TODO

\hypertarget{how-to-assess-it-15}{%
\section{How to assess it}\label{how-to-assess-it-15}}

TODO

\hypertarget{where-to-learn-more-15}{%
\section{Where to learn more}\label{where-to-learn-more-15}}

TODO

\hypertarget{random-forests}{%
\chapter{TODO: Random Forests}\label{random-forests}}

\hypertarget{tldr-16}{%
\section{TL;DR}\label{tldr-16}}

\begin{description}
\tightlist
\item[What it does]
TODO
\item[When to do it]
TODO
\item[How to do it]
TODO
\item[How to assess it]
TODO
\end{description}

\hypertarget{what-it-does-16}{%
\section{What it does}\label{what-it-does-16}}

TODO

\hypertarget{when-to-do-it-16}{%
\section{When to do it}\label{when-to-do-it-16}}

TODO

\hypertarget{how-to-do-it-16}{%
\section{How to do it}\label{how-to-do-it-16}}

TODO

\hypertarget{how-to-assess-it-16}{%
\section{How to assess it}\label{how-to-assess-it-16}}

TODO

\hypertarget{where-to-learn-more-16}{%
\section{Where to learn more}\label{where-to-learn-more-16}}

TODO

\hypertarget{boosting}{%
\chapter{TODO: Boosting}\label{boosting}}

\hypertarget{tldr-17}{%
\section{TL;DR}\label{tldr-17}}

\begin{description}
\tightlist
\item[What it does]
TODO
\item[When to do it]
TODO
\item[How to do it]
TODO
\item[How to assess it]
TODO
\end{description}

\hypertarget{what-it-does-17}{%
\section{What it does}\label{what-it-does-17}}

TODO

\hypertarget{when-to-do-it-17}{%
\section{When to do it}\label{when-to-do-it-17}}

TODO

\hypertarget{how-to-do-it-17}{%
\section{How to do it}\label{how-to-do-it-17}}

TODO

\hypertarget{how-to-assess-it-17}{%
\section{How to assess it}\label{how-to-assess-it-17}}

TODO

\hypertarget{where-to-learn-more-17}{%
\section{Where to learn more}\label{where-to-learn-more-17}}

TODO

\hypertarget{bayesian-additive-regression-trees}{%
\chapter{TODO: Bayesian Additive Regression Trees}\label{bayesian-additive-regression-trees}}

\hypertarget{tldr-18}{%
\section{TL;DR}\label{tldr-18}}

\begin{description}
\tightlist
\item[What it does]
TODO
\item[When to do it]
TODO
\item[How to do it]
TODO
\item[How to assess it]
TODO
\end{description}

\hypertarget{what-it-does-18}{%
\section{What it does}\label{what-it-does-18}}

TODO

\hypertarget{when-to-do-it-18}{%
\section{When to do it}\label{when-to-do-it-18}}

TODO

\hypertarget{how-to-do-it-18}{%
\section{How to do it}\label{how-to-do-it-18}}

TODO

\hypertarget{how-to-assess-it-18}{%
\section{How to assess it}\label{how-to-assess-it-18}}

TODO

\hypertarget{where-to-learn-more-18}{%
\section{Where to learn more}\label{where-to-learn-more-18}}

TODO

\hypertarget{support-vector-machines}{%
\chapter{TODO: Support Vector Machines}\label{support-vector-machines}}

\hypertarget{tldr-19}{%
\section{TL;DR}\label{tldr-19}}

\begin{description}
\tightlist
\item[What it does]
TODO
\item[When to do it]
TODO
\item[How to do it]
TODO
\item[How to assess it]
TODO
\end{description}

\hypertarget{what-it-does-19}{%
\section{What it does}\label{what-it-does-19}}

TODO

\hypertarget{when-to-do-it-19}{%
\section{When to do it}\label{when-to-do-it-19}}

TODO

\hypertarget{how-to-do-it-19}{%
\section{How to do it}\label{how-to-do-it-19}}

TODO

\hypertarget{how-to-assess-it-19}{%
\section{How to assess it}\label{how-to-assess-it-19}}

TODO

\hypertarget{where-to-learn-more-19}{%
\section{Where to learn more}\label{where-to-learn-more-19}}

TODO

\hypertarget{principal-component-analysis}{%
\chapter{TODO: Principal Component Analysis}\label{principal-component-analysis}}

\hypertarget{tldr-20}{%
\section{TL;DR}\label{tldr-20}}

\begin{description}
\tightlist
\item[What it does]
TODO
\item[When to do it]
TODO
\item[How to do it]
TODO
\item[How to assess it]
TODO
\end{description}

\hypertarget{what-it-does-20}{%
\section{What it does}\label{what-it-does-20}}

TODO

\hypertarget{when-to-do-it-20}{%
\section{When to do it}\label{when-to-do-it-20}}

TODO

\hypertarget{how-to-do-it-20}{%
\section{How to do it}\label{how-to-do-it-20}}

TODO

\hypertarget{how-to-assess-it-20}{%
\section{How to assess it}\label{how-to-assess-it-20}}

TODO

\hypertarget{where-to-learn-more-20}{%
\section{Where to learn more}\label{where-to-learn-more-20}}

TODO

\hypertarget{k-means-clustering}{%
\chapter{TODO: K-Means Clustering}\label{k-means-clustering}}

\hypertarget{tldr-21}{%
\section{TL;DR}\label{tldr-21}}

\begin{description}
\tightlist
\item[What it does]
TODO
\item[When to do it]
TODO
\item[How to do it]
TODO
\item[How to assess it]
TODO
\end{description}

\hypertarget{what-it-does-21}{%
\section{What it does}\label{what-it-does-21}}

TODO

\hypertarget{when-to-do-it-21}{%
\section{When to do it}\label{when-to-do-it-21}}

TODO

\hypertarget{how-to-do-it-21}{%
\section{How to do it}\label{how-to-do-it-21}}

TODO

\hypertarget{how-to-assess-it-21}{%
\section{How to assess it}\label{how-to-assess-it-21}}

TODO

\hypertarget{where-to-learn-more-21}{%
\section{Where to learn more}\label{where-to-learn-more-21}}

TODO

\hypertarget{hierarchical-clustering}{%
\chapter{TODO: Hierarchical Clustering}\label{hierarchical-clustering}}

\hypertarget{tldr-22}{%
\section{TL;DR}\label{tldr-22}}

\begin{description}
\tightlist
\item[What it does]
TODO
\item[When to do it]
TODO
\item[How to do it]
TODO
\item[How to assess it]
TODO
\end{description}

\hypertarget{what-it-does-22}{%
\section{What it does}\label{what-it-does-22}}

TODO

\hypertarget{when-to-do-it-22}{%
\section{When to do it}\label{when-to-do-it-22}}

TODO

\hypertarget{how-to-do-it-22}{%
\section{How to do it}\label{how-to-do-it-22}}

TODO

\hypertarget{how-to-assess-it-22}{%
\section{How to assess it}\label{how-to-assess-it-22}}

TODO

\hypertarget{where-to-learn-more-22}{%
\section{Where to learn more}\label{where-to-learn-more-22}}

TODO

  \bibliography{book.bib,packages.bib}

\end{document}
