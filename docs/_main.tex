% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{book}
\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={797ML Handbook},
  pdfauthor={Steve Linberg},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\usepackage{booktabs}
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage[]{natbib}
\bibliographystyle{plainnat}

\title{797ML Handbook}
\author{Steve Linberg}
\date{2022-04-17}

\begin{document}
\maketitle

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\hypertarget{about}{%
\chapter{About}\label{about}}

This book is being written as part of a final project for 797ML at UMass
Amherst, spring 2022. It contains a simple reference and breakdown for a
couple of dozen core methods used in machine learning.

The intent is twofold:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Serve as a reference for the basics of the material covered in the class, using language and examples that are as simple as possible to explain the core concepts and how to do them;
\item
  Force myself to learn these techniques better by carrying out the above.
\end{enumerate}

The main purpose of this work is to be \emph{simple}, not to be \emph{comprehensive}. We won't cover every facet of every technique, or every possible permutations of outcomes. The goal is to simply express the broad strokes and core concepts in a way that can be easily remembered, and to serve as a jumping-off point when more information is needed.

\hypertarget{authoring-guidelines}{%
\section{Authoring guidelines}\label{authoring-guidelines}}

The goal is to have no more than a few short paragraphs for each section, and to keep each explanation of the meanings of outcome variables to one sentence each.

Each chapter will have a \href{https://en.wikipedia.org/wiki/Wikipedia:Too_long;_didn\%27t_read}{TL;DR} section at the top with \emph{one-sentence answers} to following questions:

\begin{itemize}
\tightlist
\item
  What it does (answer the question starting with ``It\ldots{}'')
\item
  When to do it
\item
  How to do it
\item
  How to assess it
\end{itemize}

and it will be possible to skim through the book, just surfing the TL;DR at the top of each chapter, and get a reasonable overview without reading the rest. The rest of each chapter will deliver more fleshed-out, but \emph{short}, answers to the same questions, covering the basics and the most general concepts.

\hypertarget{contact}{%
\section{Contact}\label{contact}}

Steve Linberg: \href{mailto:steve@slinberg.net}{\nolinkurl{steve@slinberg.net}} \textbar\textbar{} \url{https://slinberg.net}

Project home: \url{https://stevelinberg.github.io/797MS-handbook/}

Github repo: \url{https://github.com/stevelinberg/797MS-handbook}

\hypertarget{resources}{%
\section{Resources}\label{resources}}

A lot of the material from this work is from the class textbook, \citet{ISLR}. I also find UNC geneticist Josh Starmer's \href{https://www.youtube.com/channel/UCtYLUTtgS3k1Fg4y5tAhLbw}{StatQuest} video series on YouTube to be immensely helpful for simple explanations of statistics and machine learning concepts. His website \href{https://statquest.org}{statquest.org} has many additional useful resources.

\hypertarget{simple-linear-regression}{%
\chapter{Simple Linear Regression}\label{simple-linear-regression}}

\hypertarget{tldr}{%
\section{TL;DR}\label{tldr}}

\begin{description}
\tightlist
\item[What it does]
Looks to see how well a single predictor variable predicts an outcome, like \emph{how well do years of education predict salary?}
\item[When to do it]
When you want to see if pretty much the simplest possible model provides enough of an explanation of variance for your purposes
\item[How to do it]
With the \texttt{lm()} function, among other ways
\item[How to assess it]
Look for a significant \(p\)-value for the predictor, and a reasonable \(R^2\)
\item[Note]
``Linear'' does not necessarily mean ``straight'' in this context; a linear regression line can curve.
\end{description}

\hypertarget{what-it-does}{%
\section{What it does}\label{what-it-does}}

Simple linear regression is where it all begins; among the simplest of all of the regression techniques in analysis, which attempts to estimate a slope and an intercept line for a set of observations using a single predictor variable \(X\) and an output variable \(Y\). It uses ordinary least squares (OLS) to build its model, looking for the line through the mean of \(X\) and \(Y\) that has the smallest sum of squares between the predicted and observed values.



\begin{figure}
\centering
\includegraphics{images/3_1.png}
\caption{\label{fig:img-slr1}Simple linear regression plot (source: \citet{ISLR}, p.~62)}
\end{figure}

The figure above shows a plot of a simple linear regression, attempting to use the variable \texttt{TV} to predict \texttt{Sales.} The blue line is the line defined by the regression with its \(Y\) intercept and slope; the red dots are the actual observations of \texttt{Sales} for each measure of the predictor \texttt{TV} on the \(X\) axis. The thin blue lines are the error in the prediction.

\hypertarget{when-to-do-it}{%
\section{When to do it}\label{when-to-do-it}}

It is a simple first step for looking at data to see if there is an easy single-variable model that does a reasonable job predicting outcomes using one predictor variable. Sometimes, it can be good enough! It has the advantage of being easy to execute, to understand and to communicate, and the value of these factors should not be underestimated. Communicating with non-specialists is an important aspect of a data scientist's job.

Linear regression requires a dataset with a continuous outcome variable; it is easiest and most effective if the predictor variable is also numeric, whether continuous or discrete. It is possible to do linear regression with non-numeric predictors, such as true/false or ordered responses, by converting the predictors to a numeric scale.

\hypertarget{how-to-do-it}{%
\section{How to do it}\label{how-to-do-it}}

This example from \citet{ISLR} shows a simple linear regression of \textbf{medv} onto \textbf{lstat}, attempting to predict median housing prices from percentage of ``lower-status'' population in the Boston data set from the late 1970s.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lm.fit }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(medv }\SpecialCharTok{\textasciitilde{}}\NormalTok{ lstat, }\AttributeTok{data =}\NormalTok{ Boston)}
\FunctionTok{plot}\NormalTok{(Boston}\SpecialCharTok{$}\NormalTok{lstat, Boston}\SpecialCharTok{$}\NormalTok{medv)}
\FunctionTok{abline}\NormalTok{(lm.fit, }\AttributeTok{lwd =} \DecValTok{3}\NormalTok{, }\AttributeTok{col =} \StringTok{"red"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{_main_files/figure-latex/unnamed-chunk-3-1.pdf}

The results of the regression are stored in the output of \texttt{lm()}, and may be viewed with \texttt{summary()}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(lm.fit)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = medv ~ lstat, data = Boston)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -15.168  -3.990  -1.318   2.034  24.500 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(>|t|)    
## (Intercept) 34.55384    0.56263   61.41   <2e-16 ***
## lstat       -0.95005    0.03873  -24.53   <2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 6.216 on 504 degrees of freedom
## Multiple R-squared:  0.5441, Adjusted R-squared:  0.5432 
## F-statistic: 601.6 on 1 and 504 DF,  p-value: < 2.2e-16
\end{verbatim}

\hypertarget{how-to-assess-it}{%
\section{How to assess it}\label{how-to-assess-it}}

The output contains a lot of information, but the key points are:

\begin{itemize}
\tightlist
\item
  \texttt{Adjusted\ R-squared}: 0.5432 is the percentage of the variation in the model that is explained by the fit's prediction, compared to just looking at how much the observations vary from their own average with no predictor variable.
\item
  \texttt{Residual\ standard\ error}: 6.216 is ``the average amount that the response will deviate from the true regression line'', or the standard deviation of the error. In this case, since the unit of \texttt{medv} is in thousands of dollars, it means the average prediction will be off by \$6,216.
\item
  \texttt{p-value}: \textless{} 2.2e-16 (basically zero) means that the model is (very) statistically significant, with a near-zero percent chance that the data in this set could have resulted from a random draw from the (unknown) population if there were no relationship between \texttt{medv} and \texttt{lstat}.
\item
  \texttt{Estimated\ intercept} of 34.55 is the \(Y\) intercept on the graph, or the estimated value of \texttt{medv} when \texttt{lstat} is zero.
\item
  \texttt{Estimated\ (lstat)}: -0.95 is the estimated coefficient of \texttt{lstat}, or the amount that \texttt{medv} changes by for each unit change in \texttt{lstat}.
\end{itemize}

The intercept and coefficient are \(\beta_0\) and \(\beta_1\), respectively, of the linear regression formula

\[\hat{y} = \hat{\beta_0} + \hat{\beta_1}x\]
Substituting the coefficients and variables, we transform this to:

\[\widehat{\text{medv}} = 34.55 - 0.95 \times \text{lstat}\]

The low p-value shows that there is (almost) definitely a relationship between \texttt{medv} and \texttt{lstat}, but the \(R^2\) of 0.54 shows that the model only explains slightly more than half of the variation in the data. It's not a bad start, but we would probably want to find a model that explains more of it.

\hypertarget{where-to-learn-more}{%
\section{Where to learn more}\label{where-to-learn-more}}

\begin{itemize}
\tightlist
\item
  Chapter 3 - 3.1 in \citet{ISLR}
\item
  \href{https://www.youtube.com/watch?v=nk2CQITm_eo}{StatQuest: Linear Regression} - this goes into good depth on the meaning of the \(F\) statistic as well, and how it used to calculate the \(p\) value.
\end{itemize}

\hypertarget{notes}{%
\section{Notes}\label{notes}}

You can also do a scatterplot in ggplot2 and add a regression line with \texttt{geom\_smooth()}; it doesn't show the coefficients or other output information, but it gives a quick visual that's a little prettier than the base R plot:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ Boston, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ lstat, }\AttributeTok{y =}\NormalTok{ medv)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{alpha =} \FloatTok{0.5}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{method =} \StringTok{"lm"}\NormalTok{, }\AttributeTok{formula =}\NormalTok{ y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x) }\SpecialCharTok{+}
  \FunctionTok{geom\_hline}\NormalTok{(}\AttributeTok{yintercept =} \FunctionTok{mean}\NormalTok{(Boston}\SpecialCharTok{$}\NormalTok{medv), }\AttributeTok{linetype =} \StringTok{"dashed"}\NormalTok{, }\AttributeTok{color =} \StringTok{"red"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_vline}\NormalTok{(}\AttributeTok{xintercept =} \FunctionTok{mean}\NormalTok{(Boston}\SpecialCharTok{$}\NormalTok{lstat), }\AttributeTok{linetype =} \StringTok{"dashed"}\NormalTok{, }\AttributeTok{color =} \StringTok{"darkgreen"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{_main_files/figure-latex/unnamed-chunk-5-1.pdf}

The means of \texttt{lstat} and \texttt{medv} are shown with dashed red and green lines, showing that the regression line goes through the center of the data.

\hypertarget{multiple-linear-regression}{%
\chapter{Multiple Linear Regression}\label{multiple-linear-regression}}

\hypertarget{tldr-1}{%
\section{TL;DR}\label{tldr-1}}

\begin{description}
\tightlist
\item[What it does]
Looks to see how well multiple predictor variables predict an outcome, like \emph{how well do years of education and age predict salary?}
\item[When to do it]
When a \protect\hyperlink{simple-linear-regression}{simple linear regression} doesn't provide a good enough explanation of variance, and you want to see if adding additional variables provides a better one
\item[How to do it]
With the \texttt{lm()} function, utilizing more than one predictor
\item[How to assess it]
Look for significant \(p\)-values for the predictors, and a reasonable adjusted-\(R^2\)
\end{description}

\hypertarget{what-it-does-1}{%
\section{What it does}\label{what-it-does-1}}

Multiple linear regression is the first natural extension of simple linear regression. It allows for more than one predictor variable to be specified. It is also possible to combine predictors in interactions, to find out if combinations of predictors have different effects than simply adding them to the model. XXX explain/demo

\hypertarget{when-to-do-it-1}{%
\section{When to do it}\label{when-to-do-it-1}}

Use multiple linear regression when a simple linear regression doesn't provide a good enough explanation of the variance you're observing, and you want to see if adding more predictors provides a better fit. Typically, this would be in response to either a low \(R^2\) that leaves a lot of unexplained variance, or even just a visual conclusion drawn from seeing a plot of a linear model with an unsatisfactory regression line.

\hypertarget{how-to-do-it-1}{%
\section{How to do it}\label{how-to-do-it-1}}

Use the same \texttt{lm()} function, with more than one predictor in the formula.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lm.fit }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(medv }\SpecialCharTok{\textasciitilde{}}\NormalTok{ lstat }\SpecialCharTok{+}\NormalTok{ age, }\AttributeTok{data =}\NormalTok{ Boston)}
\FunctionTok{plot}\NormalTok{(Boston}\SpecialCharTok{$}\NormalTok{lstat, Boston}\SpecialCharTok{$}\NormalTok{medv)}
\FunctionTok{abline}\NormalTok{(lm.fit, }\AttributeTok{lwd =} \DecValTok{3}\NormalTok{, }\AttributeTok{col =} \StringTok{"red"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{_main_files/figure-latex/unnamed-chunk-7-1.pdf}

As with simplr linear regression, the results of the regression are stored in the output of \texttt{lm()}, and may be viewed with \texttt{summary()}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(lm.fit)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = medv ~ lstat + age, data = Boston)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -15.981  -3.978  -1.283   1.968  23.158 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(>|t|)    
## (Intercept) 33.22276    0.73085  45.458  < 2e-16 ***
## lstat       -1.03207    0.04819 -21.416  < 2e-16 ***
## age          0.03454    0.01223   2.826  0.00491 ** 
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 6.173 on 503 degrees of freedom
## Multiple R-squared:  0.5513, Adjusted R-squared:  0.5495 
## F-statistic:   309 on 2 and 503 DF,  p-value: < 2.2e-16
\end{verbatim}

All of the same parameters are there, with the addition of one coefficient line for each parameter specified in the model. Each coefficient has its own estimate, standard error, \(t\) value and \(p\) value, and should be evaluated independently.

The main task is to get the ideal subset of parameters that provide the best fit, which can involve a lot of trial and error. \emph{Forward selection} involves starting with a null model (no parameters) and trying all available parameters to see which has the lowest p-value, and adding that and continuing until the parameters are no longer significant or don't reduce the adjusted \(R^2\); \emph{Backward selection} starts with all parameters and removes the least significant one at a time until all remaining parameters are significant; \emph{Mixed selection} starts with forward selection but also removes parameters that lose their significance along the way.

Other tidbits:

\begin{itemize}
\tightlist
\item
  \emph{interaction terms} can be specified with syntax like \texttt{lstat\ *\ age}, which adds \texttt{lstat}, \texttt{age} and \texttt{lstat\ *\ age} as predictors; if an interaction is significant but an individual predictor from the interaction isn't, it should still be left in the model
\item
  \emph{nonlinear transformations} like \(\text{age}^2\) can be added by escaping them inside the \texttt{I()} function, like \texttt{I(age\^{}2)}, to escape the \texttt{\^{}} character inside the formula
\end{itemize}

\hypertarget{how-to-assess-it-1}{%
\section{How to assess it}\label{how-to-assess-it-1}}

With the exception of the additional information for each parameter, the assessment is the same as for \protect\hyperlink{simple-linear-regression}{simple linear regression}.

In the example above, we see that the addition of \texttt{age} only added a tiny bit of improvement to the model from the simple linear regression on just \texttt{lstat}. Other variables, or combinations of variables, might do a better job.

\hypertarget{where-to-learn-more-1}{%
\section{Where to learn more}\label{where-to-learn-more-1}}

\begin{itemize}
\tightlist
\item
  Chapter 3.2 in \citet{ISLR}
\end{itemize}

\hypertarget{logistic-regression}{%
\chapter{Logistic Regression}\label{logistic-regression}}

\hypertarget{tldr-2}{%
\section{TL;DR}\label{tldr-2}}

\begin{description}
\tightlist
\item[What it does]
Models the \emph{probability} that an observation falls into one of two categories
\item[When to do it]
When you are trying to predict a categorical variable with two possible outcomes, like true/false, instead of something continuous like in linear regression
\item[How to do it]
With the \texttt{glm()} function, specifying \texttt{family\ =\ "binomial"}
\item[How to assess it]
Look for a significant \(p\)-value for the predictor, and assess its accuracy against training data
\end{description}

\hypertarget{what-it-does-2}{%
\section{What it does}\label{what-it-does-2}}

Logistic regression is used to make a prediction about whether an observation falls into one of two categories. This is an alternative to linear regression, which predicts a continuous outcome. A logistic regression results in a model that gives the probability of Y given X.



\begin{figure}
\includegraphics[width=0.5\linewidth]{images/4_1} \caption{Linear regression plot (source: \href{https://youtu.be/yIYKR4sgzI8?t=222}{StatQuest})}\label{fig:img-lr1}
\end{figure}

\hypertarget{when-to-do-it-2}{%
\section{When to do it}\label{when-to-do-it-2}}

As the TL;DR says: When you are trying to predict a categorical variable with two possible outcomes, like true/false, instead of something continuous.

\hypertarget{how-to-do-it-2}{%
\section{How to do it}\label{how-to-do-it-2}}

First, it's important to note that logistic regression will give better results when the model is fit using \emph{held out} or \emph{training} data, and then tested against other data, to help avoid overfitting. It is not required, but it's recommended where possible.

First, before that, fit a model using \texttt{glm()}, making sure in this case that we have a categorical value to test for:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data}\NormalTok{(Boston)}
\NormalTok{boston }\OtherTok{\textless{}{-}}\NormalTok{ Boston }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}
    \CommentTok{\# Create the categorical crim\_above\_med response variable}
    \AttributeTok{crim\_above\_med =} \FunctionTok{as.factor}\NormalTok{(}\FunctionTok{ifelse}\NormalTok{(crim }\SpecialCharTok{\textgreater{}} \FunctionTok{median}\NormalTok{(crim), }\StringTok{"Yes"}\NormalTok{, }\StringTok{"No"}\NormalTok{)),}
    \CommentTok{\# Also make a numeric version of crim\_above\_med for the plot}
    \AttributeTok{crim\_above\_med\_num =} \FunctionTok{ifelse}\NormalTok{(crim }\SpecialCharTok{\textgreater{}} \FunctionTok{median}\NormalTok{(crim), }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{)}
\NormalTok{  )}
\NormalTok{boston.fits }\OtherTok{\textless{}{-}} \FunctionTok{glm}\NormalTok{(crim\_above\_med }\SpecialCharTok{\textasciitilde{}}\NormalTok{ nox, }\AttributeTok{data =}\NormalTok{ boston, }\AttributeTok{family =}\NormalTok{ binomial)}
\FunctionTok{summary}\NormalTok{(boston.fits)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## glm(formula = crim_above_med ~ nox, family = binomial, data = boston)
## 
## Deviance Residuals: 
##      Min        1Q    Median        3Q       Max  
## -2.27324  -0.37245  -0.06847   0.39620   2.53124  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(>|z|)    
## (Intercept)  -15.818      1.386  -11.41   <2e-16 ***
## nox           29.365      2.599   11.30   <2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 701.46  on 505  degrees of freedom
## Residual deviance: 320.39  on 504  degrees of freedom
## AIC: 324.39
## 
## Number of Fisher Scoring iterations: 6
\end{verbatim}

We can see the that \(p\) value for the \texttt{nox} variable is nearly 0, suggesting a strong association between \texttt{nox} and \texttt{crim\_above\_med}.

Now split the data into training and test sets, using, say, 80\% of the data for training:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1235}\NormalTok{)}
\NormalTok{boston.training }\OtherTok{\textless{}{-}} \FunctionTok{rep}\NormalTok{(}\ConstantTok{FALSE}\NormalTok{, }\FunctionTok{nrow}\NormalTok{(boston))}
\NormalTok{boston.training[}\FunctionTok{sample}\NormalTok{(}\FunctionTok{nrow}\NormalTok{(boston), }\FunctionTok{nrow}\NormalTok{(boston) }\SpecialCharTok{*} \FloatTok{0.8}\NormalTok{)] }\OtherTok{\textless{}{-}} \ConstantTok{TRUE}
\NormalTok{boston.test }\OtherTok{\textless{}{-}} \SpecialCharTok{!}\NormalTok{boston.training}
\end{Highlighting}
\end{Shaded}

Fit the above model to the training data:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{boston.fits }\OtherTok{\textless{}{-}}
  \FunctionTok{glm}\NormalTok{(}
\NormalTok{    crim\_above\_med }\SpecialCharTok{\textasciitilde{}}\NormalTok{ nox,}
    \AttributeTok{data =}\NormalTok{ boston,}
    \AttributeTok{subset =}\NormalTok{ boston.training,}
    \AttributeTok{family =}\NormalTok{ binomial}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

And use the fit to predict the test data with \texttt{predict()}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{boston.probs }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(boston.fits, boston[boston.test,], }\AttributeTok{type =} \StringTok{"response"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{how-to-assess-it-2}{%
\section{How to assess it}\label{how-to-assess-it-2}}

Assess the accuracy of the predictions by building a confusion matrix, using the same values (``No'' and ``Yes'' in this case) as contained in the outcome variable we want to predict:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{boston.pred }\OtherTok{\textless{}{-}} \FunctionTok{rep}\NormalTok{(}\StringTok{"No"}\NormalTok{, }\FunctionTok{sum}\NormalTok{(boston.test))}
\NormalTok{boston.pred[boston.probs }\SpecialCharTok{\textgreater{}} \FloatTok{0.5}\NormalTok{] }\OtherTok{\textless{}{-}} \StringTok{"Yes"}
\FunctionTok{table}\NormalTok{(boston.pred, boston[boston.test,]}\SpecialCharTok{$}\NormalTok{crim\_above\_med)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##            
## boston.pred No Yes
##         No  36  12
##         Yes  5  49
\end{verbatim}

The above table shows that the model correctly predicted \texttt{crim\_above\_med} from \texttt{nox} 85 times out of 102, with 5 false positives and 12 false negatives. The accuracy of the model can be computed via the \texttt{mean()} of how often the predictions match the test data, a neat trick:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{mean}\NormalTok{(boston.pred }\SpecialCharTok{==}\NormalTok{ boston[boston.test,]}\SpecialCharTok{$}\NormalTok{crim\_above\_med)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.8333333
\end{verbatim}

The regression can be plotted with \texttt{ggplot2} and the \texttt{stat\_smooth()} function, utilizing the \texttt{glm} method, like so:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(boston, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ nox, }\AttributeTok{y =}\NormalTok{ crim\_above\_med\_num)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{alpha =}\NormalTok{ .}\DecValTok{5}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{stat\_smooth}\NormalTok{(}
    \AttributeTok{method =} \StringTok{"glm"}\NormalTok{,}
    \AttributeTok{formula =}\NormalTok{ y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x,}
    \AttributeTok{se =} \ConstantTok{FALSE}\NormalTok{,}
    \AttributeTok{method.args =} \FunctionTok{list}\NormalTok{(}\AttributeTok{family =}\NormalTok{ binomial),}
    \AttributeTok{col =} \StringTok{"red"}\NormalTok{,}
    \AttributeTok{lty =} \DecValTok{2}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\includegraphics{_main_files/figure-latex/unnamed-chunk-16-1.pdf}

This shows that an observation with a \texttt{nox} value of 0.6 would have around a 90\% probability of \texttt{crim\_above\_med} being true, or 1, while an observation with a \texttt{nox} value of 0.5 would only have around a 25\% probability of \texttt{crim\_above\_med} being true.

\hypertarget{where-to-learn-more-2}{%
\section{Where to learn more}\label{where-to-learn-more-2}}

\begin{itemize}
\tightlist
\item
  Chapter 4 - 4.4 in \citet{ISLR}
\item
  \href{https://www.youtube.com/watch?v=yIYKR4sgzI8}{StatQuest: Logistic Regression}
\end{itemize}

\hypertarget{notes-1}{%
\section{Notes}\label{notes-1}}

Like with linear regression, the ``multiple'' flavor of logistic regression works much the same way, with the specification of additional predictors in the formula, and so doesn't really warrant a whole chapter of its own. The selection of variables requires some effort, and is addressed under \protect\hyperlink{best_subset_selection}{best subser selection}.

\hypertarget{linear-discriminant-analysis}{%
\chapter{Linear Discriminant Analysis (LDA)}\label{linear-discriminant-analysis}}

\hypertarget{tldr-3}{%
\section{TL;DR}\label{tldr-3}}

\begin{description}
\tightlist
\item[What it does]
Separates observations into categories, like logistic regression, but using category means and variances for separation rather than probabilities
\item[When to do it]
When exploring various classifiers to see which works best for a given data set
\item[How to do it]
With the \texttt{lda()} function, using training and testing sets
\item[How to assess it]
Assess the accuracy of the predictions on the test set after training
\end{description}

\hypertarget{what-it-does-3}{%
\section{What it does}\label{what-it-does-3}}

Similar to \protect\hyperlink{principal-component-analysis}{principal component analysys} (PCA), LDA reduces dimensionality and finds the best single axis to separate two (or more) groups of observations into categories using a least-squares method of distance from a mean. Like PCA, it returns a set of new axes for the data, organized by importance, so that the first axis is the one that explains the largest amount of variance, and so on, down to \(p - 1\) axes where \(p\) is the number of categories/dimensions.

So, LDA will return 3 axes for a set of observations with 4 categories, or 1 for a set of observations with 2 categories. Each axis will have a loading score that indicates which variables had the biggest impacts on it.

\hypertarget{when-to-do-it-3}{%
\section{When to do it}\label{when-to-do-it-3}}

When you want to see if it will work better than other classification methods! It should always be tried along with other classifiers like \protect\hyperlink{logistic-regression}{logistic regression}, \protect\hyperlink{quadratic-discriminant-analysis}{quadratic discriminant analysis}, and \protect\hyperlink{naive-bayes}{Naive Bayes}.

\hypertarget{how-to-do-it-3}{%
\section{How to do it}\label{how-to-do-it-3}}

Again, using the Boston data from the \protect\hyperlink{logistic-regression}{logistic regression} chapter:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data}\NormalTok{(Boston)}
\NormalTok{boston }\OtherTok{\textless{}{-}}\NormalTok{ Boston }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}
    \CommentTok{\# Create the categorical crim\_above\_med response variable}
    \AttributeTok{crim\_above\_med =} \FunctionTok{as.factor}\NormalTok{(}\FunctionTok{ifelse}\NormalTok{(crim }\SpecialCharTok{\textgreater{}} \FunctionTok{median}\NormalTok{(crim), }\StringTok{"Yes"}\NormalTok{, }\StringTok{"No"}\NormalTok{)),}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

We again split the data into training and test sets:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1235}\NormalTok{)}
\NormalTok{boston.training }\OtherTok{\textless{}{-}} \FunctionTok{rep}\NormalTok{(}\ConstantTok{FALSE}\NormalTok{, }\FunctionTok{nrow}\NormalTok{(boston))}
\NormalTok{boston.training[}\FunctionTok{sample}\NormalTok{(}\FunctionTok{nrow}\NormalTok{(boston), }\FunctionTok{nrow}\NormalTok{(boston) }\SpecialCharTok{*} \FloatTok{0.8}\NormalTok{)] }\OtherTok{\textless{}{-}} \ConstantTok{TRUE}
\NormalTok{boston.test }\OtherTok{\textless{}{-}} \SpecialCharTok{!}\NormalTok{boston.training}
\end{Highlighting}
\end{Shaded}

And fit the above model to the training data using the \texttt{lda()} function from the \href{https://cran.r-project.org/web/packages/MASS/index.html}{MASS} library\footnote{Note to my fellow New Englanders: the MASS library is named for a book titled ``Modern Applied Statistics with S'', and although it does contain an earlier version of the Boston dataset (among many others), it has nothing to do with Massachusetts, and our Boston dataset comes from the ISLR2 library.} (note: there is no \texttt{family} argument as with \texttt{glm()}, but the calls are otherwise identical):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{boston\_lda.fits }\OtherTok{\textless{}{-}}
  \FunctionTok{lda}\NormalTok{(}
\NormalTok{    crim\_above\_med }\SpecialCharTok{\textasciitilde{}}\NormalTok{ nox,}
    \AttributeTok{data =}\NormalTok{ boston,}
    \AttributeTok{subset =}\NormalTok{ boston.training,}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\hypertarget{how-to-assess-it-3}{%
\section{How to assess it}\label{how-to-assess-it-3}}

The fit can be directly examined:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{boston\_lda.fits}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Call:
## lda(crim_above_med ~ nox, data = boston, , subset = boston.training)
## 
## Prior probabilities of groups:
##        No       Yes 
## 0.5247525 0.4752475 
## 
## Group means:
##           nox
## No  0.4710519
## Yes 0.6357865
## 
## Coefficients of linear discriminants:
##          LD1
## nox 12.29052
\end{verbatim}

And plotted:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(boston\_lda.fits)}
\end{Highlighting}
\end{Shaded}

\includegraphics{_main_files/figure-latex/unnamed-chunk-22-1.pdf}

The fit should be used to predict the test data, and the model can be assessed on its results:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{boston\_lda.pred }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(boston\_lda.fits, boston[boston.test, ])}
\FunctionTok{table}\NormalTok{(boston\_lda.pred}\SpecialCharTok{$}\NormalTok{class, boston[boston.test, ]}\SpecialCharTok{$}\NormalTok{crim\_above\_med)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      
##       No Yes
##   No  38  12
##   Yes  3  49
\end{verbatim}

In this example, LDA made the correct categorization 87 times out of 102, with 3 false positives and 12 false negatives. Again, we can compute the prediction accuracy by the mean of the correct-to-wrong guesses:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{mean}\NormalTok{(boston\_lda.pred}\SpecialCharTok{$}\NormalTok{class }\SpecialCharTok{==}\NormalTok{ boston[boston.test, ]}\SpecialCharTok{$}\NormalTok{crim\_above\_med)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.8529412
\end{verbatim}

\hypertarget{where-to-learn-more-3}{%
\section{Where to learn more}\label{where-to-learn-more-3}}

\begin{itemize}
\tightlist
\item
  Chapter 4.4.1 - 4.4.2 in \citet{ISLR}
\item
  \href{https://www.youtube.com/watch?v=azXCzI57Yfc}{StatQuest: Linear Discriminant Analysis}
\end{itemize}

\hypertarget{quadratic-discriminant-analysis}{%
\chapter{TODO: Quadratic Discriminant Analysis}\label{quadratic-discriminant-analysis}}

\hypertarget{tldr-4}{%
\section{TL;DR}\label{tldr-4}}

\begin{description}
\tightlist
\item[What it does]
TODO
\item[When to do it]
TODO
\item[How to do it]
TODO
\item[How to assess it]
TODO
\end{description}

\hypertarget{what-it-does-4}{%
\section{What it does}\label{what-it-does-4}}

TODO

\hypertarget{when-to-do-it-4}{%
\section{When to do it}\label{when-to-do-it-4}}

TODO

\hypertarget{how-to-do-it-4}{%
\section{How to do it}\label{how-to-do-it-4}}

TODO

\hypertarget{how-to-assess-it-4}{%
\section{How to assess it}\label{how-to-assess-it-4}}

TODO

\hypertarget{where-to-learn-more-4}{%
\section{Where to learn more}\label{where-to-learn-more-4}}

TODO

\hypertarget{naive-bayes}{%
\chapter{TODO: Naive Bayes}\label{naive-bayes}}

\hypertarget{tldr-5}{%
\section{TL;DR}\label{tldr-5}}

\begin{description}
\tightlist
\item[What it does]
TODO
\item[When to do it]
TODO
\item[How to do it]
TODO
\item[How to assess it]
TODO
\end{description}

\hypertarget{what-it-does-5}{%
\section{What it does}\label{what-it-does-5}}

TODO

\hypertarget{when-to-do-it-5}{%
\section{When to do it}\label{when-to-do-it-5}}

TODO

\hypertarget{how-to-do-it-5}{%
\section{How to do it}\label{how-to-do-it-5}}

TODO

\hypertarget{how-to-assess-it-5}{%
\section{How to assess it}\label{how-to-assess-it-5}}

TODO

\hypertarget{where-to-learn-more-5}{%
\section{Where to learn more}\label{where-to-learn-more-5}}

TODO

\hypertarget{k-nearest-neighbors}{%
\chapter{TODO: K-Nearest Neighbors}\label{k-nearest-neighbors}}

\hypertarget{tldr-6}{%
\section{TL;DR}\label{tldr-6}}

\begin{description}
\tightlist
\item[What it does]
TODO
\item[When to do it]
TODO
\item[How to do it]
TODO
\item[How to assess it]
TODO
\end{description}

\hypertarget{what-it-does-6}{%
\section{What it does}\label{what-it-does-6}}

TODO

\hypertarget{when-to-do-it-6}{%
\section{When to do it}\label{when-to-do-it-6}}

TODO

\hypertarget{how-to-do-it-6}{%
\section{How to do it}\label{how-to-do-it-6}}

TODO

\hypertarget{how-to-assess-it-6}{%
\section{How to assess it}\label{how-to-assess-it-6}}

TODO

\hypertarget{where-to-learn-more-6}{%
\section{Where to learn more}\label{where-to-learn-more-6}}

TODO

\hypertarget{poisson-regression}{%
\chapter{TODO: Poisson Regression}\label{poisson-regression}}

\hypertarget{tldr-7}{%
\section{TL;DR}\label{tldr-7}}

\begin{description}
\tightlist
\item[What it does]
TODO
\item[When to do it]
TODO
\item[How to do it]
TODO
\item[How to assess it]
TODO
\end{description}

\hypertarget{what-it-does-7}{%
\section{What it does}\label{what-it-does-7}}

TODO

\hypertarget{when-to-do-it-7}{%
\section{When to do it}\label{when-to-do-it-7}}

TODO

\hypertarget{how-to-do-it-7}{%
\section{How to do it}\label{how-to-do-it-7}}

TODO

\hypertarget{how-to-assess-it-7}{%
\section{How to assess it}\label{how-to-assess-it-7}}

TODO

\hypertarget{where-to-learn-more-7}{%
\section{Where to learn more}\label{where-to-learn-more-7}}

TODO

\hypertarget{cross-validation}{%
\chapter{TODO: Cross-Validation}\label{cross-validation}}

\hypertarget{tldr-8}{%
\section{TL;DR}\label{tldr-8}}

\begin{description}
\tightlist
\item[What it does]
TODO
\item[When to do it]
TODO
\item[How to do it]
TODO
\item[How to assess it]
TODO
\end{description}

\hypertarget{what-it-does-8}{%
\section{What it does}\label{what-it-does-8}}

TODO

\hypertarget{when-to-do-it-8}{%
\section{When to do it}\label{when-to-do-it-8}}

TODO

\hypertarget{how-to-do-it-8}{%
\section{How to do it}\label{how-to-do-it-8}}

TODO

\hypertarget{how-to-assess-it-8}{%
\section{How to assess it}\label{how-to-assess-it-8}}

TODO

\hypertarget{where-to-learn-more-8}{%
\section{Where to learn more}\label{where-to-learn-more-8}}

TODO

\hypertarget{bootstrap}{%
\chapter{TODO: Bootstrap}\label{bootstrap}}

\hypertarget{tldr-9}{%
\section{TL;DR}\label{tldr-9}}

\begin{description}
\tightlist
\item[What it does]
TODO
\item[When to do it]
TODO
\item[How to do it]
TODO
\item[How to assess it]
TODO
\end{description}

\hypertarget{what-it-does-9}{%
\section{What it does}\label{what-it-does-9}}

TODO

\hypertarget{when-to-do-it-9}{%
\section{When to do it}\label{when-to-do-it-9}}

TODO

\hypertarget{how-to-do-it-9}{%
\section{How to do it}\label{how-to-do-it-9}}

TODO

\hypertarget{how-to-assess-it-9}{%
\section{How to assess it}\label{how-to-assess-it-9}}

TODO

\hypertarget{where-to-learn-more-9}{%
\section{Where to learn more}\label{where-to-learn-more-9}}

TODO

\hypertarget{best-subset-selection}{%
\chapter{TODO: Best Subset Selection}\label{best-subset-selection}}

\hypertarget{tldr-10}{%
\section{TL;DR}\label{tldr-10}}

\begin{description}
\tightlist
\item[What it does]
TODO
\item[When to do it]
TODO
\item[How to do it]
TODO
\item[How to assess it]
TODO
\end{description}

\hypertarget{what-it-does-10}{%
\section{What it does}\label{what-it-does-10}}

TODO

\hypertarget{when-to-do-it-10}{%
\section{When to do it}\label{when-to-do-it-10}}

TODO

\hypertarget{how-to-do-it-10}{%
\section{How to do it}\label{how-to-do-it-10}}

TODO

\hypertarget{how-to-assess-it-10}{%
\section{How to assess it}\label{how-to-assess-it-10}}

TODO

\hypertarget{where-to-learn-more-10}{%
\section{Where to learn more}\label{where-to-learn-more-10}}

TODO

\hypertarget{stepwise-selection}{%
\chapter{TODO: Stepwise Selection}\label{stepwise-selection}}

\hypertarget{tldr-11}{%
\section{TL;DR}\label{tldr-11}}

\begin{description}
\tightlist
\item[What it does]
TODO
\item[When to do it]
TODO
\item[How to do it]
TODO
\item[How to assess it]
TODO
\end{description}

\hypertarget{what-it-does-11}{%
\section{What it does}\label{what-it-does-11}}

TODO

\hypertarget{when-to-do-it-11}{%
\section{When to do it}\label{when-to-do-it-11}}

TODO

\hypertarget{how-to-do-it-11}{%
\section{How to do it}\label{how-to-do-it-11}}

TODO

\hypertarget{how-to-assess-it-11}{%
\section{How to assess it}\label{how-to-assess-it-11}}

TODO

\hypertarget{where-to-learn-more-11}{%
\section{Where to learn more}\label{where-to-learn-more-11}}

TODO

\hypertarget{ridge-regression}{%
\chapter{TODO: Ridge Regression}\label{ridge-regression}}

\hypertarget{tldr-12}{%
\section{TL;DR}\label{tldr-12}}

\begin{description}
\tightlist
\item[What it does]
TODO
\item[When to do it]
TODO
\item[How to do it]
TODO
\item[How to assess it]
TODO
\end{description}

\hypertarget{what-it-does-12}{%
\section{What it does}\label{what-it-does-12}}

TODO

\hypertarget{when-to-do-it-12}{%
\section{When to do it}\label{when-to-do-it-12}}

TODO

\hypertarget{how-to-do-it-12}{%
\section{How to do it}\label{how-to-do-it-12}}

TODO

\hypertarget{how-to-assess-it-12}{%
\section{How to assess it}\label{how-to-assess-it-12}}

TODO

\hypertarget{where-to-learn-more-12}{%
\section{Where to learn more}\label{where-to-learn-more-12}}

TODO

\hypertarget{lasso}{%
\chapter{TODO: Lasso}\label{lasso}}

\hypertarget{tldr-13}{%
\section{TL;DR}\label{tldr-13}}

\begin{description}
\tightlist
\item[What it does]
TODO
\item[When to do it]
TODO
\item[How to do it]
TODO
\item[How to assess it]
TODO
\end{description}

\hypertarget{what-it-does-13}{%
\section{What it does}\label{what-it-does-13}}

TODO

\hypertarget{when-to-do-it-13}{%
\section{When to do it}\label{when-to-do-it-13}}

TODO

\hypertarget{how-to-do-it-13}{%
\section{How to do it}\label{how-to-do-it-13}}

TODO

\hypertarget{how-to-assess-it-13}{%
\section{How to assess it}\label{how-to-assess-it-13}}

TODO

\hypertarget{where-to-learn-more-13}{%
\section{Where to learn more}\label{where-to-learn-more-13}}

TODO

\hypertarget{principal-component-regression}{%
\chapter{TODO: Principal Component Regression}\label{principal-component-regression}}

\hypertarget{tldr-14}{%
\section{TL;DR}\label{tldr-14}}

\begin{description}
\tightlist
\item[What it does]
TODO
\item[When to do it]
TODO
\item[How to do it]
TODO
\item[How to assess it]
TODO
\end{description}

\hypertarget{what-it-does-14}{%
\section{What it does}\label{what-it-does-14}}

TODO

\hypertarget{when-to-do-it-14}{%
\section{When to do it}\label{when-to-do-it-14}}

TODO

\hypertarget{how-to-do-it-14}{%
\section{How to do it}\label{how-to-do-it-14}}

TODO

\hypertarget{how-to-assess-it-14}{%
\section{How to assess it}\label{how-to-assess-it-14}}

TODO

\hypertarget{where-to-learn-more-14}{%
\section{Where to learn more}\label{where-to-learn-more-14}}

TODO

\hypertarget{bagging}{%
\chapter{TODO: Bagging}\label{bagging}}

\hypertarget{tldr-15}{%
\section{TL;DR}\label{tldr-15}}

\begin{description}
\tightlist
\item[What it does]
TODO
\item[When to do it]
TODO
\item[How to do it]
TODO
\item[How to assess it]
TODO
\end{description}

\hypertarget{what-it-does-15}{%
\section{What it does}\label{what-it-does-15}}

TODO

\hypertarget{when-to-do-it-15}{%
\section{When to do it}\label{when-to-do-it-15}}

TODO

\hypertarget{how-to-do-it-15}{%
\section{How to do it}\label{how-to-do-it-15}}

TODO

\hypertarget{how-to-assess-it-15}{%
\section{How to assess it}\label{how-to-assess-it-15}}

TODO

\hypertarget{where-to-learn-more-15}{%
\section{Where to learn more}\label{where-to-learn-more-15}}

TODO

\hypertarget{random-forests}{%
\chapter{TODO: Random Forests}\label{random-forests}}

\hypertarget{tldr-16}{%
\section{TL;DR}\label{tldr-16}}

\begin{description}
\tightlist
\item[What it does]
TODO
\item[When to do it]
TODO
\item[How to do it]
TODO
\item[How to assess it]
TODO
\end{description}

\hypertarget{what-it-does-16}{%
\section{What it does}\label{what-it-does-16}}

TODO

\hypertarget{when-to-do-it-16}{%
\section{When to do it}\label{when-to-do-it-16}}

TODO

\hypertarget{how-to-do-it-16}{%
\section{How to do it}\label{how-to-do-it-16}}

TODO

\hypertarget{how-to-assess-it-16}{%
\section{How to assess it}\label{how-to-assess-it-16}}

TODO

\hypertarget{where-to-learn-more-16}{%
\section{Where to learn more}\label{where-to-learn-more-16}}

TODO

\hypertarget{boosting}{%
\chapter{TODO: Boosting}\label{boosting}}

\hypertarget{tldr-17}{%
\section{TL;DR}\label{tldr-17}}

\begin{description}
\tightlist
\item[What it does]
TODO
\item[When to do it]
TODO
\item[How to do it]
TODO
\item[How to assess it]
TODO
\end{description}

\hypertarget{what-it-does-17}{%
\section{What it does}\label{what-it-does-17}}

TODO

\hypertarget{when-to-do-it-17}{%
\section{When to do it}\label{when-to-do-it-17}}

TODO

\hypertarget{how-to-do-it-17}{%
\section{How to do it}\label{how-to-do-it-17}}

TODO

\hypertarget{how-to-assess-it-17}{%
\section{How to assess it}\label{how-to-assess-it-17}}

TODO

\hypertarget{where-to-learn-more-17}{%
\section{Where to learn more}\label{where-to-learn-more-17}}

TODO

\hypertarget{bayesian-additive-regression-trees}{%
\chapter{TODO: Bayesian Additive Regression Trees}\label{bayesian-additive-regression-trees}}

\hypertarget{tldr-18}{%
\section{TL;DR}\label{tldr-18}}

\begin{description}
\tightlist
\item[What it does]
TODO
\item[When to do it]
TODO
\item[How to do it]
TODO
\item[How to assess it]
TODO
\end{description}

\hypertarget{what-it-does-18}{%
\section{What it does}\label{what-it-does-18}}

TODO

\hypertarget{when-to-do-it-18}{%
\section{When to do it}\label{when-to-do-it-18}}

TODO

\hypertarget{how-to-do-it-18}{%
\section{How to do it}\label{how-to-do-it-18}}

TODO

\hypertarget{how-to-assess-it-18}{%
\section{How to assess it}\label{how-to-assess-it-18}}

TODO

\hypertarget{where-to-learn-more-18}{%
\section{Where to learn more}\label{where-to-learn-more-18}}

TODO

\hypertarget{support-vector-machines}{%
\chapter{TODO: Support Vector Machines}\label{support-vector-machines}}

\hypertarget{tldr-19}{%
\section{TL;DR}\label{tldr-19}}

\begin{description}
\tightlist
\item[What it does]
TODO
\item[When to do it]
TODO
\item[How to do it]
TODO
\item[How to assess it]
TODO
\end{description}

\hypertarget{what-it-does-19}{%
\section{What it does}\label{what-it-does-19}}

TODO

\hypertarget{when-to-do-it-19}{%
\section{When to do it}\label{when-to-do-it-19}}

TODO

\hypertarget{how-to-do-it-19}{%
\section{How to do it}\label{how-to-do-it-19}}

TODO

\hypertarget{how-to-assess-it-19}{%
\section{How to assess it}\label{how-to-assess-it-19}}

TODO

\hypertarget{where-to-learn-more-19}{%
\section{Where to learn more}\label{where-to-learn-more-19}}

TODO

\hypertarget{principal-component-analysis}{%
\chapter{TODO: Principal Component Analysis}\label{principal-component-analysis}}

\hypertarget{tldr-20}{%
\section{TL;DR}\label{tldr-20}}

\begin{description}
\tightlist
\item[What it does]
TODO
\item[When to do it]
TODO
\item[How to do it]
TODO
\item[How to assess it]
TODO
\end{description}

\hypertarget{what-it-does-20}{%
\section{What it does}\label{what-it-does-20}}

TODO

\hypertarget{when-to-do-it-20}{%
\section{When to do it}\label{when-to-do-it-20}}

TODO

\hypertarget{how-to-do-it-20}{%
\section{How to do it}\label{how-to-do-it-20}}

TODO

\hypertarget{how-to-assess-it-20}{%
\section{How to assess it}\label{how-to-assess-it-20}}

TODO

\hypertarget{where-to-learn-more-20}{%
\section{Where to learn more}\label{where-to-learn-more-20}}

TODO

\hypertarget{k-means-clustering}{%
\chapter{TODO: K-Means Clustering}\label{k-means-clustering}}

\hypertarget{tldr-21}{%
\section{TL;DR}\label{tldr-21}}

\begin{description}
\tightlist
\item[What it does]
TODO
\item[When to do it]
TODO
\item[How to do it]
TODO
\item[How to assess it]
TODO
\end{description}

\hypertarget{what-it-does-21}{%
\section{What it does}\label{what-it-does-21}}

TODO

\hypertarget{when-to-do-it-21}{%
\section{When to do it}\label{when-to-do-it-21}}

TODO

\hypertarget{how-to-do-it-21}{%
\section{How to do it}\label{how-to-do-it-21}}

TODO

\hypertarget{how-to-assess-it-21}{%
\section{How to assess it}\label{how-to-assess-it-21}}

TODO

\hypertarget{where-to-learn-more-21}{%
\section{Where to learn more}\label{where-to-learn-more-21}}

TODO

\hypertarget{hierarchical-clustering}{%
\chapter{TODO: Hierarchical Clustering}\label{hierarchical-clustering}}

\hypertarget{tldr-22}{%
\section{TL;DR}\label{tldr-22}}

\begin{description}
\tightlist
\item[What it does]
TODO
\item[When to do it]
TODO
\item[How to do it]
TODO
\item[How to assess it]
TODO
\end{description}

\hypertarget{what-it-does-22}{%
\section{What it does}\label{what-it-does-22}}

TODO

\hypertarget{when-to-do-it-22}{%
\section{When to do it}\label{when-to-do-it-22}}

TODO

\hypertarget{how-to-do-it-22}{%
\section{How to do it}\label{how-to-do-it-22}}

TODO

\hypertarget{how-to-assess-it-22}{%
\section{How to assess it}\label{how-to-assess-it-22}}

TODO

\hypertarget{where-to-learn-more-22}{%
\section{Where to learn more}\label{where-to-learn-more-22}}

TODO

  \bibliography{book.bib,packages.bib}

\end{document}
