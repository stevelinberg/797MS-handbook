# Linear Discriminant Analysis (LDA) {#linear-discriminant-analysis}

```{r include=FALSE}
library(tidyverse)
library(ISLR2)
library(MASS)
```

## TL;DR

What it does
: Separates observations into categories, like logistic regression, but using category means and variances for separation rather than probabilities

When to do it
: When exploring various classifiers to see which works best for a given data set

How to do it
: With the `lda()` function, using training and testing sets

How to assess it
: Assess the accuracy of the predictions on the test set after training

## What it does

Similar to [principal component analysys](#principal-component-analysis) (PCA), LDA reduces dimensionality and finds the best single axis to separate two (or more) groups of observations into categories using a least-squares method of distance from a mean. Like PCA, it returns a set of new axes for the data, organized by importance, so that the first axis is the one that explains the largest amount of variance, and so on, down to $p - 1$ axes where $p$ is the number of categories/dimensions.

So, LDA will return 3 axes for a set of observations with 4 categories, or 1 for a set of observations with 2 categories. Each axis will have a loading score that indicates which variables had the biggest impacts on it.

## When to do it

When you want to see if it will work better than other classification methods! It should always be tried along with other classifiers like [logistic regression](#logistic-regression), [quadratic discriminant analysis](#quadratic-discriminant-analysis), and [Naive Bayes](#naive-bayes).

## How to do it

Again, using the Boston data from the [logistic regression](#logistic-regression) chapter:

```{r}
data(Boston)
boston <- Boston %>%
  mutate(
    # Create the categorical crim_above_med response variable
    crim_above_med = as.factor(ifelse(crim > median(crim), "Yes", "No")),
    # Also make a numeric version of crim_above_med for the plot
    crim_above_med_num = ifelse(crim > median(crim), 1, 0)
  )
```

We again split the data into training and test sets:

```{r}
set.seed(1235)
boston.training <- rep(FALSE, nrow(boston))
boston.training[sample(nrow(boston), nrow(boston) * 0.8)] <- TRUE
boston.test <- !boston.training
```

And fit the above model to the training data using the `lda()` function (note: there is no `family` argument as with `glm()`, but the calls are otherwise identical):

```{r}
boston_lda.fits <-
  lda(
    crim_above_med ~ nox,
    data = boston,
    subset = boston.training,
  )
```

## How to assess it

The fit can be directly examined:

```{r}
boston_lda.fits
```

And plotted:

```{r}
plot(boston_lda.fits)
```

The fit should be used to predict the test data, and the model can be assessed on its results:

```{r}
boston_lda.pred <- predict(boston_lda.fits, boston[boston.test,])
boston_lda.class <- boston_lda.pred$class
table(boston_lda.class, boston[boston.test,]$crim_above_med)
```

In this example, LDA made the correct categorization 87 times out of 102, with 3 false positives and 12 false negatives. Again, we can compute the prediction accuracy by the mean of the correct-to-wrong guesses:

```{r}
mean(boston_lda.class == boston[boston.test,]$crim_above_med)
```

<!--
XXX need something on interpreting the output beyond just the accuracy.
-->

## Where to learn more

- Chapter 4.4.1 - 4.4.2 in @ISLR
- [StatQuest: Linear Discriminant Analysis](https://www.youtube.com/watch?v=azXCzI57Yfc)


